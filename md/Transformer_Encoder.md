# Word Embedding ä¸åºåˆ—å»ºæ¨¡ï¼šä»£ç é€è¡Œè¯¦è§£ä¸å®æˆ˜ç¬”è®°

> é€‚ç”¨åœºæ™¯ï¼šç†è§£å¦‚ä½•ä»â€œè¯è¡¨ç´¢å¼•åºåˆ—â€å¾—åˆ°â€œç¨ å¯†å‘é‡è¡¨ç¤ºï¼ˆembeddingï¼‰â€ï¼Œå¹¶ä¸ºåç»­çš„ RNN/Transformer ç­‰åºåˆ—æ¨¡å‹åšè¾“å…¥å‡†å¤‡ã€‚

---

## 1. èƒŒæ™¯ä¸ç›®æ ‡

æˆ‘ä»¬æœ‰**æºåºåˆ—**ï¼ˆsrcï¼‰ä¸**ç›®æ ‡åºåˆ—**ï¼ˆtgtï¼‰ã€‚æ¯ä¸ªåºåˆ—ç”±**è¯è¡¨ç´¢å¼•**ç»„æˆï¼ˆæ•´æ•° IDï¼‰ï¼Œé•¿åº¦ä¸ä¸€ã€‚ä¸ºäº†å¹¶è¡Œè®­ç»ƒï¼Œéœ€è¦ï¼š

1. æŠŠä¸åŒé•¿åº¦çš„åºåˆ—åš **padding** åˆ°ç»Ÿä¸€é•¿åº¦ï¼ˆå¸¸ç”¨ padding å€¼æ˜¯ `0`ï¼‰ï¼›  
2. ç”¨ `nn.Embedding` æŠŠç´¢å¼•åºåˆ—å˜ä¸º**ç¨ å¯†å‘é‡åºåˆ—**ï¼ˆå½¢çŠ¶ä¸€èˆ¬æ˜¯ `[batch, seq_len, model_dim]`ï¼‰ã€‚

æœ¬ç¬”è®°è¯¦ç»†è§£é‡Šä»¥ä¸‹ä»£ç ï¼ˆç•¥æœ‰æ¶¦è‰²æ’ç‰ˆï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# å…³äºword embeddingï¼Œä»¥åºåˆ—å»ºæ¨¡ä¸ºä¾‹
# æ„å»ºåºåˆ—ï¼Œåºåˆ—çš„å­—ç¬¦ä»¥å…¶åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•çš„å½¢å¼è¡¨ç¤º
batch_size = 2

# å•è¯è¡¨å¤§å°
max_num_src_words = 8
max_num_tgt_words = 8

# åºåˆ—çš„æœ€å¤§é•¿åº¦
max_src_seq_len = 5
max_tgt_seq_len = 5

# ç»´åº¦
model_dim = 8

# æŠŠ æºåºåˆ— å’Œ ç›®æ ‡åºåˆ— çš„é•¿åº¦å®šä¸‹æ¥
src_len = torch.Tensor([2, 4]).to(torch.int32)
tgt_len = torch.Tensor([4, 3]).to(torch.int32)

# å•è¯ç´¢å¼•æ„æˆçš„å¥å­, æ„å»º batchï¼Œå¹¶ä¸”åšäº† padding, é»˜è®¤å€¼ä¸º 0
src_seq = torch.cat([
    torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len - L)), 0)
    for L in src_len
])
tgt_seq = torch.cat([
    torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len - L)), 0)
    for L in tgt_len
])

# æ„é€  embeddingï¼ˆæ³¨æ„ï¼šå¯é€‰ padding_idx=0ï¼‰
src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)
tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)
src_embedding = src_embedding_table(src_seq)
tgt_embedding = tgt_embedding_table(tgt_seq)
```

---

## 2. å…³é”®è¶…å‚æ•°

- `batch_size = 2`ï¼šä¸€ä¸ª batch æœ‰ä¸¤æ¡æ ·æœ¬ã€‚
- è¯è¡¨å¤§å°ï¼š
  - `max_num_src_words = 8`
  - `max_num_tgt_words = 8`
  - **æ³¨æ„**ï¼šåé¢ `nn.Embedding(max_num_*_words + 1, ...)` å¤šåŠ äº† `1`ï¼Œé€šå¸¸æ˜¯ä¸ºäº†**é¢„ç•™ index=0 ä½œä¸º padding**ï¼ˆä¹Ÿæ–¹ä¾¿æœªæ¥è¯è¡¨æ‰©å……ï¼‰ã€‚
- åºåˆ—æœ€å¤§é•¿åº¦ï¼š`max_src_seq_len = max_tgt_seq_len = 5`ã€‚
- å‘é‡ç»´åº¦ï¼š`model_dim = 8`ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ª token ä¼šè¢«æ˜ å°„åˆ°é•¿åº¦ä¸º 8 çš„å‘é‡ã€‚

---

## 3. åºåˆ—çœŸå®é•¿åº¦ï¼ˆæœª padding å‰ï¼‰

```python
src_len = torch.Tensor([2, 4]).to(torch.int32)
tgt_len = torch.Tensor([4, 3]).to(torch.int32)
```

- `src_len` è¡¨ç¤ºæœ¬ batch çš„ä¸¤æ¡ **æºåºåˆ—** çœŸæ­£é•¿åº¦åˆ†åˆ«ä¸º 2ã€4ã€‚  
- `tgt_len` è¡¨ç¤ºä¸¤æ¡ **ç›®æ ‡åºåˆ—** çœŸæ­£é•¿åº¦åˆ†åˆ«ä¸º 4ã€3ã€‚  
- è¿™äº›é•¿åº¦ä»…ç”¨æ¥æŒ‡å¯¼åç»­**éšæœºç”Ÿæˆç´¢å¼•åºåˆ—**ä¸**padding**ï¼Œæœ¬èº«ä¸ä¼šè¢«ç›´æ¥å–‚ç»™ Embeddingã€‚

> å°æç¤ºï¼šé•¿åº¦å¼ é‡ç”¨ `int32` æˆ– `int64` éƒ½è¡Œï¼›çœŸæ­£ç”¨äºç´¢å¼•çš„å¼ é‡å¿…é¡»æ˜¯ **é•¿æ•´å‹ï¼ˆ`torch.long` / `int64`ï¼‰**ã€‚

---

## 4. ç”Ÿæˆæ‰¹åºåˆ—å¹¶åš Padding

ä»¥ `src_seq` ä¸ºä¾‹ï¼ˆ`tgt_seq` åŒç†ï¼‰ï¼š

```python
src_seq = torch.cat([
    torch.unsqueeze(
        F.pad(
            torch.randint(1, max_num_src_words, (L,)),  # ç”Ÿæˆé•¿åº¦ä¸º L çš„éšæœºç´¢å¼•åºåˆ—
            (0, max_src_seq_len - L)                    # åœ¨å³ä¾§ pad åˆ°ç»Ÿä¸€é•¿åº¦
        ),
        0  # åœ¨ç¬¬ 0 ç»´ï¼ˆbatch ç»´ï¼‰å¢åŠ ä¸€ç»´ï¼Œå½¢çŠ¶ä» [L] -> [1, max_src_seq_len]
    )
    for L in src_len
])
```

é€æ­¥ç†è§£ï¼š

1. **éšæœºç´¢å¼•åºåˆ—**ï¼š`torch.randint(1, max_num_src_words, (L,))`  
   - å–å€¼èŒƒå›´æ˜¯ `[1, max_num_src_words-1]`ï¼ˆä¸Šç•Œå¼€åŒºé—´ï¼‰ï¼Œå³ `[1, 7]`ã€‚  
   - è¿™æ ·å¯ä»¥**é¿å… 0**ï¼ˆç•™ç»™ paddingï¼‰ã€‚
2. **Padding**ï¼š`F.pad(x, (0, max_len - L))`  
   - å¯¹ 1D åºåˆ—ï¼Œ`(left, right)` è¡¨ç¤ºåœ¨æœ€åä¸€ç»´å·¦/å³ä¾§åˆ†åˆ«è¡¥å¤šå°‘ä¸ªå€¼ï¼Œé»˜è®¤è¡¥ `0`ã€‚  
   - è¿™é‡Œåœ¨**å³ä¾§**è¡¥ `max_len - L` ä¸ª `0`ï¼Œè®©åºåˆ—å˜æˆç»Ÿä¸€é•¿åº¦ `max_len=5`ã€‚
3. **Unsqueeze + Cat**ï¼š  
   - `unsqueeze(..., 0)` æŠŠå½¢çŠ¶ `[max_len]` å˜æˆ `[1, max_len]`ï¼Œæ–¹ä¾¿åç»­æŒ‰ batch ç»´æ‹¼æ¥ã€‚  
   - `torch.cat([...])` æŠŠä¸¤æ¡æ ·æœ¬åœ¨ç¬¬ 0 ç»´æ‹¼èµ·æ¥ï¼Œæœ€ç»ˆ `src_seq` å½¢çŠ¶æ˜¯ **`[batch_size, max_src_seq_len] = [2, 5]`**ã€‚

`tgt_seq` åŒç†ï¼Œæœ€ç»ˆå½¢çŠ¶ä¹Ÿæ˜¯ `[2, 5]`ã€‚åºåˆ—ä¸­çš„ 0 éƒ½æ˜¯ padding ä½ç½®ã€‚

---

## 5. æ„é€  Embedding è¡¨å¹¶æŸ¥è¡¨

```python
src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)
tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)

src_embedding = src_embedding_table(src_seq)  # [2, 5, 8]
tgt_embedding = tgt_embedding_table(tgt_seq)  # [2, 5, 8]
```

- `nn.Embedding(num_embeddings, embedding_dim)` ä¼šåˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º `[num_embeddings, embedding_dim]` çš„**æŸ¥æ‰¾è¡¨**ã€‚  
- è¾“å…¥æ˜¯**ç´¢å¼•å¼ é‡**ï¼ˆ`long` ç±»å‹ï¼Œå½¢å¦‚ `[2, 5]`ï¼‰ï¼Œè¾“å‡ºæ˜¯å¯¹åº”çš„**å‘é‡å¼ é‡**ï¼ˆ`float` ç±»å‹ï¼Œå½¢å¦‚ `[2, 5, 8]`ï¼‰ã€‚  
- è¿™é‡ŒæŠŠ `num_embeddings` è®¾ä¸º `max_num_*_words + 1`ï¼Œä¸º index=0 çš„ padding é¢„ç•™ç©ºé—´ã€‚

> **å¼ºçƒˆå»ºè®®**ï¼šåœ¨ `Embedding` é‡Œæ˜¾å¼æŒ‡å®š `padding_idx=0`ï¼š
>
> ```python
> nn.Embedding(max_num_src_words + 1, model_dim, padding_idx=0)
> ```
>
> è¿™æ ·ï¼Œpadding çš„è¡Œå‘é‡åœ¨è®­ç»ƒä¸­ä¸ä¼šè¢«æ›´æ–°ï¼ˆæ¢¯åº¦ä¸º 0ï¼‰ï¼Œå¯é¿å… â€œæ¨¡å‹æŠŠæ³¨æ„åŠ›å­¦åˆ° padding ä¸Šâ€ã€‚

---

## 6. å½¢çŠ¶ä¸æ•°æ®ç±»å‹æ€»è§ˆ

| åç§° | ä½œç”¨ | å½¢çŠ¶ | dtype | å¤‡æ³¨ |
|---|---|---|---|---|
| `src_len` | æºåºåˆ—çœŸå®é•¿åº¦ | `[2]` | `int32` | å€¼ä¸º `[2, 4]` |
| `tgt_len` | ç›®æ ‡åºåˆ—çœŸå®é•¿åº¦ | `[2]` | `int32` | å€¼ä¸º `[4, 3]` |
| `src_seq` | æºç´¢å¼•åºåˆ—ï¼ˆå·² paddingï¼‰ | `[2, 5]` | `long` | å–å€¼èŒƒå›´å« 0ï¼ˆpaddingï¼‰ä¸ 1~7 |
| `tgt_seq` | ç›®æ ‡ç´¢å¼•åºåˆ—ï¼ˆå·² paddingï¼‰ | `[2, 5]` | `long` | åŒä¸Š |
| `src_embedding` | æºåºåˆ—çš„å‘é‡è¡¨ç¤º | `[2, 5, 8]` | `float32` | æ¯ä¸ª token â†’ 8 ç»´å‘é‡ |
| `tgt_embedding` | ç›®æ ‡åºåˆ—çš„å‘é‡è¡¨ç¤º | `[2, 5, 8]` | `float32` | åŒä¸Š |

---

## 7. éšæœºæ€§çš„è¯´æ˜ä¸å¯å¤ç°å®éªŒ

ç”±äºä½¿ç”¨äº† `torch.randint(...)`ï¼Œæ¯æ¬¡è¿è¡Œä¼šå¾—åˆ°ä¸åŒçš„ç´¢å¼•åºåˆ—ã€‚è‹¥å¸Œæœ›**å¤ç°å®éªŒ**ï¼Œå¯åœ¨ç”Ÿæˆå‰è®¾ç½®éšæœºç§å­ï¼š

```python
torch.manual_seed(42)
```

---

## 8. å¸¸è§å‘ä½ä¸è°ƒè¯•å»ºè®®

1. **ç´¢å¼•è¶Šç•Œ**ï¼šå¦‚æœ `nn.Embedding(num_embeddings=9, ...)`ï¼Œå…è®¸çš„ç´¢å¼•æ˜¯ `0~8`ã€‚è‹¥ä½ ç”¨åˆ°äº† `9` ä¼šæŠ¥é”™ã€‚  
   - æœ¬ä¾‹ä¸­ `torch.randint(1, 8, ...)` çš„ä¸Šç•Œæ˜¯å¼€åŒºé—´ï¼Œä¸ä¼šç”Ÿæˆ `8`ï¼Œå› æ­¤å®‰å…¨ã€‚
2. **dtype ä¸åŒ¹é…**ï¼šä¼ ç»™ `Embedding` çš„è¾“å…¥å¿…é¡»æ˜¯ `long`ã€‚å¦‚æœä½ ä»åˆ«å¤„æ‹¿çš„å¼ é‡æ˜¯ `int32/float`ï¼Œéœ€ `.long()`ã€‚  
3. **å¿˜è®° `padding_idx`**ï¼šè™½ç„¶ä¸æ˜¯ç¡¬æ€§é”™è¯¯ï¼Œä½†æœ€å¥½è®¾ç½® `padding_idx=0`ï¼Œé¿å…æ¨¡å‹æŠŠ padding å­¦å‡ºéé›¶å‘é‡ã€‚  
4. **mask çš„ä½¿ç”¨**ï¼šä¸‹æ¸¸ï¼ˆå¦‚ Transformerï¼‰éœ€è¦åˆ©ç”¨é•¿åº¦æˆ– 0 å€¼ä½ç½®å»æ„é€  **padding mask**ï¼Œå±è”½æ³¨æ„åŠ›ã€‚

---

## 9. è¿›é˜¶ï¼šç”±é•¿åº¦æˆ– 0 å€¼æ„é€  Padding Mask

å¦‚æœä½ ä¿ç•™äº† `src_len`ï¼Œå¯ä»¥ç›´æ¥åŸºäºé•¿åº¦æ„é€  maskï¼ˆ`True` è¡¨ç¤ºæ˜¯ padding ä½ç½®ï¼Œéœ€è¦è¢«â€œé®ä½â€ï¼‰ï¼š

```python
# åŸºäºé•¿åº¦æ„é€  key padding mask: [batch, seq_len]
batch_indices = torch.arange(max_src_seq_len).unsqueeze(0)  # [1, seq_len]
src_key_padding_mask = batch_indices >= src_len.unsqueeze(1)  # [2, 5], bool

# æˆ–è€…ï¼šåŸºäºç´¢å¼•æ˜¯å¦ä¸º 0 æ¥æ„é€ 
src_key_padding_mask_alt = (src_seq == 0)  # [2, 5], bool
```

åœ¨ PyTorch çš„ `nn.Transformer` ä¸­ï¼Œ`src_key_padding_mask` å¯ç›´æ¥ä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œç”¨äºæ³¨æ„åŠ›è®¡ç®—æ—¶å±è”½ paddingã€‚

---

## 10. å®Œæ•´å¯å¤ç°ç¤ºä¾‹ï¼ˆæ”¹è¿›ç‰ˆï¼Œå« padding_idx ä¸ maskï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

torch.manual_seed(42)  # å¤ç°

batch_size = 2
max_num_src_words = 8
max_num_tgt_words = 8
max_src_seq_len = 5
max_tgt_seq_len = 5
model_dim = 8

src_len = torch.tensor([2, 4], dtype=torch.int32)
tgt_len = torch.tensor([4, 3], dtype=torch.int32)

# ç”Ÿæˆå¹¶ padding
def make_batch(lengths, max_len, vocab_high_exclusive):
    seqs = []
    for L in lengths.tolist():
        # 1~(vocab_high_exclusive-1)ï¼Œé¿å… 0ï¼ˆç•™ç»™ paddingï¼‰
        x = torch.randint(1, vocab_high_exclusive, (L,), dtype=torch.long)
        x = F.pad(x, (0, max_len - L), value=0)  # å³ä¾§è¡¥ 0
        x = x.unsqueeze(0)  # [1, max_len]
        seqs.append(x)
    return torch.cat(seqs, dim=0)  # [batch, max_len]

src_seq = make_batch(src_len, max_src_seq_len, max_num_src_words)
tgt_seq = make_batch(tgt_len, max_tgt_seq_len, max_num_tgt_words)

# Embeddingï¼ˆæ˜¾å¼æŒ‡å®š padding_idx=0ï¼‰
src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim, padding_idx=0)
tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim, padding_idx=0)

src_embedding = src_embedding_table(src_seq)  # [2, 5, 8]
tgt_embedding = tgt_embedding_table(tgt_seq)  # [2, 5, 8]

# ç”±é•¿åº¦æ„é€  padding maskï¼ˆTrue è¡¨ç¤º paddingï¼‰
idx = torch.arange(max_src_seq_len).unsqueeze(0)
src_key_padding_mask = idx >= src_len.unsqueeze(1)  # [2, 5], bool

print("src_seq:\n", src_seq)
print("src_embedding shape:", src_embedding.shape)
print("src_key_padding_mask:\n", src_key_padding_mask)
```

---

## 11. å°ç»“

- ç”¨ `randint + pad + unsqueeze + cat` å¯ä»¥ä»**ä¸åŒé•¿åº¦çš„ç´¢å¼•åºåˆ—**æ„é€ å‡º**å®šé•¿æ‰¹åºåˆ—**ï¼›  
- ç”¨ `nn.Embedding` æŠŠ**ç´¢å¼•**è½¬æˆ**ç¨ å¯†å‘é‡**ï¼Œå½¢çŠ¶ä¸º `[batch, seq_len, model_dim]`ï¼›  
- è®°å¾—ä¸º padding é¢„ç•™ç´¢å¼• `0`ï¼Œå¹¶è®¾ç½® `padding_idx=0`ï¼Œä¸‹æ¸¸ç”¨ **padding mask** å±è”½æ— æ•ˆä½ç½®ã€‚

ç¥ä½ åœ¨åç»­çš„ RNN/Transformer ä¸­ç©å¾—å¼€å¿ƒï¼ğŸš€
