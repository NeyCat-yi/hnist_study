# æ‰‹å†™æ•°å­—è¯†åˆ«

## å¯¼åŒ…

```python
import pandas as pd # å¤„ç†æ•°æ®
from sklearn.model_selection import train_test_split # åˆ’åˆ†æ•°æ®é›† pip install scikit-learn
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import transforms
import numpy as np
```

## å¯¼å…¥æ•°æ®

### è¯»å– CSV æ–‡ä»¶

```python
df = pd.read_csv('datasets/origin.csv')
```

### ä½¿ç”¨ sklearn è¿›è¡Œæ‹†åˆ†

```python
# test_size=0.2  -> è¡¨ç¤ºæµ‹è¯•é›†å  20%ï¼Œè®­ç»ƒé›†è‡ªåŠ¨å  80%
# random_state=42 -> éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç åˆ‡åˆ†çš„ç»“æœéƒ½ä¸€æ ·ï¼ˆå¯æ¢æˆä»»æ„æ•°å­—ï¼‰
# shuffle=True   -> é»˜è®¤å°±æ˜¯ Trueï¼Œè¡¨ç¤ºä¼šå…ˆéšæœºæ‰“ä¹±æ•°æ®å†åˆ‡åˆ†
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)
```

## æ•°æ®é¢„å¤„ç†

### è‡ªå®šä¹‰ Dataset ç±»

```python
class CSVDataset(Dataset):
    def __init__(self, dataframe, target_column=None, transform=None):
        """
        transform: ä¼ å…¥ä¸€ä¸ª torchvision.transforms å¯¹è±¡
        """
        self.transform = transform
        
        # 1. å‡†å¤‡æ•°æ®
        if target_column:
            self.X = dataframe.drop(columns=[target_column]).values
            self.y = dataframe[target_column].values
            self.has_target = True
        else:
            self.X = dataframe.values
            self.has_target = False

        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿ç•™åŸå§‹æ•°æ®æ ¼å¼ (numpy uint8)ï¼Œä¸è¦æ€¥ç€è½¬ Tensor æˆ–é™¤ä»¥ 255
        # è¿™ä¸€æ­¥äº¤ç»™ transform å»åš
        self.X = self.X.astype(np.uint8) 

        if self.has_target:
            self.y = torch.tensor(self.y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        # å–å‡ºä¸€è¡Œæ•°æ® (numpy array)
        data = self.X[idx]
        
        # -----------------------------------------------------------
        # ğŸ”¥ å…³é”®ç‚¹ï¼šä¸ºäº†ä½¿ç”¨ PyTorch å¼ºå¤§çš„å›¾åƒå¤„ç†åº“ï¼Œ
        # æˆ‘ä»¬é€šå¸¸å…ˆæŠŠå¹³é“ºçš„ 784 è¿˜åŸæˆ 28x28 çš„å›¾ç‰‡å½¢çŠ¶
        # -----------------------------------------------------------
        # å¦‚æœä½ çš„æ•°æ®ä¸æ˜¯å›¾ç‰‡ï¼Œå¯ä»¥è·³è¿‡ reshapeï¼Œä½† torchvision é€šå¸¸å–œæ¬¢ HxWxC æ ¼å¼
        data = data.reshape(28, 28) 
        
        # ğŸ”¥ è°ƒç”¨ transform (å¦‚æœåœ¨å¤–éƒ¨å®šä¹‰äº†çš„è¯)
        if self.transform:
            data = self.transform(data)
            
        if self.has_target:
            return data, self.y[idx]
        else:
            return data
```

### å®šä¹‰ Transform(ç»Ÿä¸€åšæ•°æ®é¢„å¤„ç†)

```python
# å®šä¹‰å¤„ç†æµç¨‹
data_transform = transforms.Compose([
    # 1. è‡ªåŠ¨è½¬ Tensor å¹¶ä¸”å½’ä¸€åŒ–åˆ° [0, 1] (å³è‡ªåŠ¨é™¤ä»¥ 255)
    transforms.ToTensor(),
    
    # 2. (å¯é€‰) æ ‡å‡†åŒ–ï¼š(x - mean) / std
    # MNIST çš„ç»å…¸å‡å€¼å’Œæ–¹å·®æ˜¯ 0.1307 å’Œ 0.3081
    # åŠ ä¸Šè¿™è¡Œä¼šè®©æ¨¡å‹æ”¶æ•›æ›´å¿«ï¼Œä½†ä¸æ˜¯å¿…é¡»çš„
    transforms.Normalize(mean=(0.1307,), std=(0.3081,)),
    
    # 3. å› ä¸º ToTensor æŠŠæ•°æ®å˜æˆäº† (1, 28, 28) çš„ 3D å½¢çŠ¶
    # è€Œä½ çš„ MLP è¾“å…¥å±‚éœ€è¦ (784) çš„ 1D å½¢çŠ¶
    # æ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠå®ƒå‹æ‰ (Flatten)
    transforms.Lambda(lambda x: x.view(-1))
])
```



### å®ä¾‹åŒ– Dataset ä¸ DataLoader

```python
batch_size = 32
# å®ä¾‹åŒ– Datasetï¼ŒæŠŠ transform ä¼ è¿›å»
train_dataset = CSVDataset(
    train_df, 
    target_column='label', 
    transform=data_transform  # <--- æ³¨å…¥çµé­‚
)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # batch_size è‡ªè¡Œå®šä¹‰

# --- éªŒè¯ä¸€ä¸‹ ---
features, labels = next(iter(train_loader))

print(features.shape) 
# åº”è¯¥æ˜¯ [32, 784] -> æˆåŠŸè¢«å‹æ‰å› MLP éœ€è¦çš„æ ¼å¼
print(features.max()) 
# åº”è¯¥æ˜¯ 1.0 å·¦å³ (å¦‚æœç”¨äº† Normalize å¯èƒ½ä¼šå¤§äº 1ï¼Œå¦‚æœåªç”¨ ToTensor åˆ™æ˜¯ 1.0)
print(features.min())

# B. å®ä¾‹åŒ– DataLoader
# batch_size=2: æ¯æ¬¡åå‡º 2 æ¡æ•°æ®
# shuffle=True: æ¯ä¸ª epoch éƒ½ä¼šé‡æ–°æ‰“ä¹±æ•°æ®ï¼ˆè®­ç»ƒé›†é€šå¸¸éœ€è¦ Trueï¼‰

test_dataset = CSVDataset(
    test_df, 
    target_column='label', 
    transform=data_transform  # <--- æ³¨å…¥çµé­‚
)

test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) # batch_size è‡ªè¡Œå®šä¹‰
```

### æµ‹è¯•

```python
# æµ‹è¯• DataLoader
features, labels = next(iter(train_loader))

print("ç‰¹å¾å½¢çŠ¶:", features.shape) # åº”è¯¥æ˜¯ [32, 784]
print("æ ‡ç­¾å½¢çŠ¶:", labels.shape)   # åº”è¯¥æ˜¯ [32]
print("æµ‹è¯•é€šè¿‡ï¼")

# --- æµ‹è¯•ä¸€ä¸‹ DataLoader ---
print("å¼€å§‹è¯»å– Batch æ•°æ®:")
for batch_idx, (features, labels) in enumerate(train_loader):
    print(f"\nBatch {batch_idx + 1}:")
    print(f"ç‰¹å¾ Tensor å½¢çŠ¶: {features.shape}")
    print(f"æ ‡ç­¾ Tensor å½¢çŠ¶: {labels.shape}")
    print("ç‰¹å¾æ•°æ®:", features)
    print("æ ‡ç­¾æ•°æ®:", labels)
```

## æ­å»ºæ¨¡å‹

### ä¸€ä¸ªç®€å•çš„ MLP

```python
class DigitRecognitionMLP(nn.Module):
    def __init__(self):
        super(DigitRecognitionMLP, self).__init__()
        
        # 1. è¾“å…¥å±‚ -> éšè—å±‚ 1
        # è¾“å…¥ç»´åº¦ 784 (ä»£è¡¨ 28x28 çš„åƒç´ ç‚¹)
        # è¾“å‡ºç»´åº¦ 512 (è¿™æ˜¯ä¸€ä¸ªç»éªŒå€¼ï¼Œå¯ä»¥è°ƒæ•´)
        self.fc1 = nn.Linear(in_features=784, out_features=512)
        
        # 2. éšè—å±‚ 1 -> éšè—å±‚ 2
        self.fc2 = nn.Linear(512, 256)
        
        # 3. éšè—å±‚ 2 -> è¾“å‡ºå±‚
        # è¾“å‡ºç»´åº¦ 10 (ä»£è¡¨æ•°å­— 0-9ï¼Œå…± 10 ä¸ªç±»åˆ«)
        self.fc3 = nn.Linear(256, 10)
        
        # å®šä¹‰ Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ (å¯é€‰ï¼Œä½†æ¨è)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x çš„å½¢çŠ¶å¯èƒ½æ˜¯ (batch_size, 1, 28, 28) æˆ–è€… (batch_size, 784)
        
        # 1. å±•å¹³ (Flatten)
        # å¦‚æœè¾“å…¥æ˜¯å›¾ç‰‡ (N, 1, 28, 28)ï¼Œè¿™æ­¥æŠŠå®ƒå˜æˆ (N, 784)
        # å¦‚æœè¾“å…¥å·²ç»æ˜¯ CSV è¯»å‡ºæ¥çš„ (N, 784)ï¼Œè¿™æ­¥ä¸ä¼šæ”¹å˜æ•°æ®ï¼Œå¾ˆå®‰å…¨
        x = x.view(-1, 784)
        
        # 2. ç¬¬ä¸€å±‚è¿ç®— + æ¿€æ´»å‡½æ•° (ReLU)
        x = F.relu(self.fc1(x))
        x = self.dropout(x) # éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œé˜²æ­¢æ­»è®°ç¡¬èƒŒ
        
        # 3. ç¬¬äºŒå±‚è¿ç®— + æ¿€æ´»å‡½æ•°
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        
        # 4. è¾“å‡ºå±‚
        # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦åŠ  Softmaxï¼Œå› ä¸º PyTorch çš„ CrossEntropyLoss è‡ªå¸¦äº† Softmax
        x = self.fc3(x)
        
        return x
```

### è®­ç»ƒä»£ç 

```python
# ==========================================
# 1. é…ç½®è®¾å¤‡ (CPU è¿˜æ˜¯ GPU?)
# ==========================================
# å¦‚æœæœ‰æ˜¾å¡(NVIDIA)ï¼Œå°±ç”¨ cudaï¼›å¦‚æœæ˜¯ Mac M1/M2ï¼Œå¯ä»¥ç”¨ mpsï¼›å¦åˆ™ç”¨ cpu
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
print(f"æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: {device}")

# ==========================================
# 2. å‡†å¤‡æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
# ==========================================
# å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
model = DigitRecognitionMLP().to(device)

# å®šä¹‰æŸå¤±å‡½æ•° (å¤šåˆ†ç±»é—®é¢˜æ ‡å‡†é…ç½®)
criterion = nn.CrossEntropyLoss()

# å®šä¹‰ä¼˜åŒ–å™¨ (Adam æ˜¯æœ€å¸¸ç”¨çš„ï¼Œå­¦ä¹ ç‡è®¾ä¸º 0.001)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================================
# 3. å¼€å§‹è®­ç»ƒå¾ªç¯
# ==========================================
num_epochs = 10  # è®­ç»ƒå¤šå°‘è½® (éå†æ•´ä¸ªè®­ç»ƒé›† 10 æ¬¡)

for epoch in range(num_epochs):
    
    # --- é˜¶æ®µ A: è®­ç»ƒ (Training) ---
    model.train()  # å¯ç”¨ Dropout å’Œ BatchNorm
    running_loss = 0.0
    
    for i, (images, labels) in enumerate(train_loader):
        # 1. æŠŠæ•°æ®ç§»åˆ° GPU/CPU
        images = images.to(device)
        labels = labels.to(device) # æ ‡ç­¾å¿…é¡»æ˜¯ long ç±»å‹
        
        # 2. æ¸…ç©ºæ¢¯åº¦ (é˜²æ­¢æ¢¯åº¦ç´¯åŠ )
        optimizer.zero_grad()
        
        # 3. å‰å‘ä¼ æ’­ (Forward pass)
        outputs = model(images)
        
        # 4. è®¡ç®—æŸå¤± (Loss)
        loss = criterion(outputs, labels)
        
        # 5. åå‘ä¼ æ’­ (Backward pass)
        loss.backward()
        
        # 6. æ›´æ–°å‚æ•° (Update weights)
        optimizer.step()
        
        running_loss += loss.item()
    
    # è®¡ç®—æœ¬è½®å¹³å‡è®­ç»ƒæŸå¤±
    avg_train_loss = running_loss / len(train_loader)

    # --- é˜¶æ®µ B: æµ‹è¯•/éªŒè¯ (Validation) ---
    model.eval()  # å…³é—­ Dropoutï¼Œä¿è¯æµ‹è¯•ç»“æœç¨³å®š
    correct = 0
    total = 0
    
    # torch.no_grad() è¡¨ç¤ºè¿™éƒ¨åˆ†ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜å’Œæ—¶é—´
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            
            # è·å–é¢„æµ‹ç»“æœ: max è¿”å› (æœ€å¤§å€¼, æœ€å¤§å€¼çš„ç´¢å¼•)
            # æˆ‘ä»¬åªéœ€è¦ç´¢å¼• (predicted)ï¼Œå®ƒä»£è¡¨é¢„æµ‹çš„æ•°å­— (0-9)
            _, predicted = torch.max(outputs.data, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
    accuracy = 100 * correct / total
    
    # --- æ‰“å°æœ¬è½®ç»“æœ ---
    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"| è®­ç»ƒ Loss: {avg_train_loss:.4f} "
          f"| æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")

print("è®­ç»ƒå®Œæˆï¼")
```

# Titanic

## å¯¼åŒ…

```python
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

```

## æ•°æ®ç†è§£ä¸æ ¸å¿ƒç­–ç•¥

æ³°å¦å°¼å…‹å·çš„é»„é‡‘æ³•åˆ™ï¼š**â€œWomen and children firstâ€ (å¥³å£«å’Œå„¿ç«¥ä¼˜å…ˆ)**ã€‚

1. **å…³é”®ç‰¹å¾**ï¼š
   - **Sex (æ€§åˆ«)**ï¼šå¥³æ€§å¹¸å­˜ç‡æé«˜ã€‚
   - **Pclass (èˆ¹ç¥¨ç­‰çº§)**ï¼šå¤´ç­‰èˆ±ï¼ˆå¯Œäººï¼‰å¹¸å­˜ç‡æ›´é«˜ã€‚
   - **Age (å¹´é¾„)**ï¼šå°å­©å¹¸å­˜ç‡é«˜ã€‚
   - **SibSp / Parch (äº²å±)**ï¼šç‹¬èº«ä¸€äººæˆ–å¤§å®¶åº­å¯èƒ½å­˜æ´»ç‡ä½ï¼Œä¸­ç­‰å®¶åº­å­˜æ´»ç‡é«˜ã€‚
2. **æ•°æ®é™·é˜±**ï¼š
   - **Age**ï¼šæœ‰å¾ˆå¤šç¼ºå¤±å€¼ï¼Œå¿…é¡»å¡«å……ï¼ˆæ¯”å¦‚ç”¨å¹³å‡å€¼ï¼‰ã€‚
   - **Cabin**ï¼šç¼ºå¤±å¤ªå¤šï¼Œé€šå¸¸ç›´æ¥ä¸¢å¼ƒæˆ–åªä¿ç•™â€œæ˜¯å¦æœ‰å®¢èˆ±â€è¿™ä¸ªä¿¡æ¯ã€‚
   - **Name**ï¼šçœ‹èµ·æ¥æ²¡ç”¨ï¼Œä½†å¯ä»¥æå–å‡ºç§°å‘¼ï¼ˆMr, Mrs, Miss, Masterï¼‰ï¼Œè¿™éšå«äº†ç¤¾ä¼šåœ°ä½å’Œå¹´é¾„ä¿¡æ¯ã€‚

## æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹

```python
# 1. è¯»å–æ•°æ®
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# ä¸ºäº†ç»Ÿä¸€å¤„ç†ï¼Œå…ˆåˆå¹¶ï¼Œå¤„ç†å®Œå†æ‹†å¼€
# test_df é‡Œé¢æ²¡æœ‰ 'Survived' åˆ—ï¼Œå…ˆè¡¥ä¸ªå ä½ç¬¦
test_df['Survived'] = np.nan
all_data = pd.concat([train_df, test_df], ignore_index=True)

# --- 2. ç‰¹å¾å·¥ç¨‹ (ä¿®å¤ç‰ˆ) ---

# A. æå–ç§°å‘¼ (Title) å¹¶æ˜ å°„
all_data['Title'] = all_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())
title_map = {
    "Mr": "Mr", "Miss": "Miss", "Mrs": "Mrs", "Master": "Master",
    "Dr": "Officer", "Rev": "Officer", "Col": "Officer", "Major": "Officer",
}
# æ³¨æ„ï¼šåŒ¹é…ä¸åˆ°çš„å¡«ä¸º 'Rare'ï¼Œé˜²æ­¢äº§ç”Ÿ NaN
all_data['Title'] = all_data['Title'].map(title_map).fillna("Rare")

# B. å¡«å……ç¼ºå¤±å€¼
all_data['Age'] = all_data.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))
all_data['Fare'] = all_data['Fare'].fillna(all_data['Fare'].median())
all_data['Embarked'] = all_data['Embarked'].fillna('S')

# C. åˆ›é€ å®¶åº­ç‰¹å¾
all_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1

# ğŸ”¥ ä¿®å¤é‡ç‚¹ 1ï¼šåšå†³åˆ æ‰æ‰€æœ‰åŸå§‹å­—ç¬¦ä¸²åˆ— ğŸ”¥
# Name, Ticket, Cabin æ˜¯æœ€å®¹æ˜“å¯¼è‡´ object é”™è¯¯çš„ç½ªé­ç¥¸é¦–
cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']
all_data = all_data.drop(columns=cols_to_drop)

# D. One-Hot Encoding (æŠŠå‰©ä¸‹çš„åˆ†ç±»å˜é‡è½¬æ•°å­—)
# Pclass è™½ç„¶æ˜¯æ•°å­— 1,2,3ï¼Œä½†ä¹Ÿå»ºè®®å½“ä½œåˆ†ç±»å˜é‡å¤„ç†
cat_cols = ['Pclass', 'Sex', 'Embarked', 'Title']
all_data = pd.get_dummies(all_data, columns=cat_cols)

# ğŸ”¥ ä¿®å¤é‡ç‚¹ 2ï¼šå¼ºåˆ¶ç±»å‹è½¬æ¢æ£€æŸ¥ ğŸ”¥
# è¿™ä¸€æ­¥æ˜¯ä¸ºäº†â€œæ‰åè›‹â€ï¼Œå¦‚æœè¿˜æœ‰å­—ç¬¦ä¸²æ··åœ¨é‡Œé¢ï¼Œè¿™é‡Œä¼šç«‹åˆ»æŠ¥é”™å¹¶å‘Šè¯‰ä½ å“ªä¸€åˆ—æœ‰é—®é¢˜
features_check = all_data.drop(columns=['Survived'])
try:
    # å°è¯•å¼ºåˆ¶è½¬ä¸º floatï¼Œå¦‚æœæŠ¥é”™è¯´æ˜è¿˜æœ‰è„æ•°æ®
    features_check = features_check.astype(float)
except ValueError as e:
    print("âŒ æ•°æ®æ¸…æ´—å¤±è´¥ï¼ä»¥ä¸‹åˆ—åŒ…å«äº†éæ•°å­—å­—ç¬¦ï¼Œè¯·æ£€æŸ¥ï¼š")
    # æ‰“å°å‡ºæ‰€æœ‰éæ•°å­—çš„åˆ—å
    print(all_data.select_dtypes(include=['object']).columns)
    raise e

# E. å½’ä¸€åŒ– (Age, Fare, FamilySize)
# è¿™ä¸€æ­¥å¿…é¡»åœ¨å…¨éƒ¨è½¬ä¸ºæ•°å­—ååš
for col in ['Age', 'Fare', 'FamilySize']:
    mean = all_data[col].mean()
    std = all_data[col].std()
    if std != 0:
        all_data[col] = (all_data[col] - mean) / std
    else:
        all_data[col] = 0

# --- æ‹†åˆ†å›è®­ç»ƒ/æµ‹è¯•é›† ---
train_processed = all_data[all_data['Survived'].notnull()].copy()
test_processed = all_data[all_data['Survived'].isnull()].drop(columns=['Survived']).copy()

# ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°
train_processed['Survived'] = train_processed['Survived'].astype(int)
```

## å®šä¹‰ Dataset 

```python
# --- 3. Dataset å®šä¹‰ (æ›´åŠ ç¨³å¥çš„å†™æ³•) ---
class TitanicDataset(Dataset):
    def __init__(self, dataframe, target_column=None):
        if target_column:
            # .values å¯èƒ½ä¼šè¿”å› object ç±»å‹ï¼Œå¿…é¡»æ˜¾å¼ .astype(float)
            self.X = dataframe.drop(columns=[target_column]).values.astype(np.float32)
            self.y = dataframe[target_column].values.astype(np.int64)
            self.has_target = True
        else:
            self.X = dataframe.values.astype(np.float32)
            self.has_target = False

        # è½¬ä¸º PyTorch Tensor
        self.X = torch.from_numpy(self.X)
        if self.has_target:
            self.y = torch.from_numpy(self.y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        if self.has_target:
            return self.X[idx], self.y[idx]
        else:
            return self.X[idx]

# --- å®ä¾‹åŒ–æµ‹è¯• ---
train_dataset = TitanicDataset(train_processed, target_column='Survived')
test_dataset = TitanicDataset(test_processed, target_column=None)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
# æ‰“å°ä¸€ä¸‹çœ‹çœ‹æ˜¯å¦æˆåŠŸ
sample_x, sample_y = train_dataset[0]
print("âœ… æ•°æ®è½¬æ¢æˆåŠŸï¼")
print(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {len(sample_x)}") # åº”è¯¥åœ¨ 20 å·¦å³
print(f"ç¬¬ä¸€æ¡æ•°æ®ç‰¹å¾: {sample_x}")
```

## æ¨¡å‹æ­å»º

```python
class TitanicNet(nn.Module):
    def __init__(self, input_size):
        super(TitanicNet, self).__init__()
        # 3å±‚ç½‘ç»œè¶³çŸ£
        self.fc1 = nn.Linear(input_size, 64)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2) # é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 2)    # è¾“å‡º 0(æ­») æˆ– 1(ç”Ÿ)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# åŠ¨æ€è·å–è¾“å…¥ç»´åº¦ (æ ¹æ®ä½ ç‰¹å¾å·¥ç¨‹åçš„åˆ—æ•°)
input_dim = train_processed.shape[1] - 1 # å‡å» Survived åˆ—
model = TitanicNet(input_dim)
```

## è®­ç»ƒä¸ç”Ÿæˆæäº¤æ–‡ä»¶

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# --- è®­ç»ƒ ---
epochs = 50
for epoch in range(epochs):
    model.train()
    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# --- é¢„æµ‹å¹¶ç”Ÿæˆ CSV ---
model.eval()
predictions = []

with torch.no_grad():
    for features in test_loader:
        features = features.to(device)
        outputs = model(features)
        _, predicted = torch.max(outputs, 1)
        predictions.extend(predicted.cpu().numpy())

# è¯»å–åŸå§‹ test.csv åªæ˜¯ä¸ºäº†æ‹¿ PassengerId
original_test = pd.read_csv('test.csv')

submission = pd.DataFrame({
    'PassengerId': original_test['PassengerId'],
    'Survived': predictions
})

submission.to_csv('titanic_submission.csv', index=False)
print("æ–‡ä»¶å·²ç”Ÿæˆï¼štitanic_submission.csv")
```

# Classify_leaves

## å¯¼åŒ…

```python
import pandas as pd
import os
from torch.utils.data import Dataset
from PIL import Image
import torch
import torchvision.models as models
import torch.nn as nn
from torchvision import transforms
import torch.optim as optim
from tqdm import tqdm  # <--- è¿›åº¦æ¡
import torchvision.models as models
```

## å®šä¹‰ Dataset

```python
class LeavesDataset(Dataset):
    def __init__(self, csv_path, file_root, mode='train', valid_ratio=0.2, resize_height=224, resize_width=224, transforms=None):
        """
        mode: 'train', 'valid', æˆ– 'test'
        """
        self.file_root = file_root
        self.transforms = transforms
        
        # 1. è¯»å– CSV
        self.data = pd.read_csv(csv_path)
        
        # 2. å¤„ç†æ ‡ç­¾ (å¦‚æœæ˜¯è®­ç»ƒ/éªŒè¯é›†)
        if mode != 'test':
            # è·å–æ‰€æœ‰å”¯ä¸€çš„ç±»åˆ«åç§°ï¼Œå¹¶æ’åºä¿è¯é¡ºåºå›ºå®š
            unique_labels = sorted(self.data['label'].unique())
            # ç”Ÿæˆå­—å…¸: {'label_name': 0, ...}
            self.class_to_num = {label: i for i, label in enumerate(unique_labels)}
            # ç”Ÿæˆåå‘å­—å…¸ (ç”¨äºæœ€åç”Ÿæˆæäº¤æ–‡ä»¶): {0: 'label_name', ...}
            self.num_to_class = {i: label for i, label in enumerate(unique_labels)}
            
            # åˆ©ç”¨ map å°†å­—ç¬¦ä¸²è½¬ä¸ºæ•°å­— (ä½ åˆšæ‰é—®çš„ map ç”¨æ³•ï¼) æ–°åŠ äº†ä¸€åˆ—
            self.data['label_num'] = self.data['label'].map(self.class_to_num)

        # 3. ç®€å•çš„åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ç”¨ K-Fold)
        data_len = len(self.data)
        train_len = int(data_len * (1 - valid_ratio))
        
        if mode == 'train':
            self.data = self.data.iloc[:train_len]
        elif mode == 'valid':
            self.data = self.data.iloc[train_len:]
        # test æ¨¡å¼è¯»å–æ•´ä¸ª submission.csvï¼Œä¸éœ€è¦åˆ‡åˆ†

        self.mode = mode

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # 1. æ‹¼æ¥è·¯å¾„ (ä½ åˆšæ‰é—®çš„ os.path.join)
        img_path = os.path.join(self.file_root, self.data.iloc[idx, 0])
        
        # 2. è¯»å–å›¾ç‰‡
        image = Image.open(img_path)
        
        # 3. åº”ç”¨æ•°æ®å¢å¼º (Transforms)
        if self.transforms:
            image = self.transforms(image)
            
        # 4. è¿”å›ç»“æœ
        if self.mode == 'test':
            return image # æµ‹è¯•é›†æ²¡æœ‰æ ‡ç­¾
        else:
            # å–å‡ºæ•°å­—æ ‡ç­¾ (ä½ åˆšæ‰é—®çš„ iloc)
            label = self.data.iloc[idx]['label_num']
            # print(self.data.iloc[idx])  # è°ƒè¯•ä¿¡æ¯ï¼ŒæŸ¥çœ‹å½“å‰è¡Œæ•°æ®
            return image, label
```

## å®šä¹‰æ¨¡å‹

```python
# 1. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
model = models.resnet50(pretrained=True)

# 2. ä¿®æ”¹æœ€åä¸€å±‚å…¨è¿æ¥å±‚ (fc)
# ResNet50 çš„æœ€åä¸€å±‚è¾“å…¥æ˜¯ 2048ï¼Œæˆ‘ä»¬è¦è¾“å‡º 176 ç±»
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 176) 

# 3. æ”¾åˆ° GPU ä¸Š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
```



## æ•°æ®å¢å¼º Transforms

```python
# è®­ç»ƒé›†å¢å¼º
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),    # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomVerticalFlip(),      # éšæœºå‚ç›´ç¿»è½¬ (æ ‘å¶æœä¸Šæœä¸‹éƒ½è¡Œ)
    transforms.ColorJitter(brightness=0.5, contrast=0.5), # éšæœºæ”¹äº®åº¦å¯¹æ¯”åº¦
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet æ ‡å‡†å½’ä¸€åŒ–
])

# éªŒè¯/æµ‹è¯•é›† (ä¸åšå¢å¼ºï¼Œåªåšæ ‡å‡†åŒ–)
val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
```

## å®ä¾‹åŒ– Dataset å’Œ DataLoader

```python
train_dataset = LeavesDataset(
    csv_path='datasets/train.csv',
    file_root='datasets',
    mode='train',
    transforms=train_transforms
)

val_dataset = LeavesDataset(
    csv_path='datasets/train.csv',
    file_root='datasets',
    mode='valid',
    transforms=val_transforms
)

train_dataloder = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
)
val_dataloader = DataLoader(
    val_dataset,
    batch_size=32,
    shuffle=False,
)
```

## æµ‹è¯•ä¸€ä¸‹ DataLoader

```python
# --- æµ‹è¯•ä¸€ä¸‹ DataLoader ---
print("å¼€å§‹è¯»å– Batch æ•°æ®:")
for batch_idx, (features, labels) in enumerate(train_dataloder):
    print(f"\nBatch {batch_idx + 1}:")
    print(f"ç‰¹å¾ Tensor å½¢çŠ¶: {features.shape}")
    print(f"æ ‡ç­¾ Tensor å½¢çŠ¶: {labels.shape}")
    print("ç‰¹å¾æ•°æ®:", features)
    print("æ ‡ç­¾æ•°æ®:", labels)
```



## è®­ç»ƒ

```python
# ==========================================
# 1. é…ç½®è®¾å¤‡
# ==========================================
# device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
print(f"æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: {device}")

# ==========================================
# 2. å‡†å¤‡æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
# ==========================================
# å‡è®¾ model, train_dataloder, val_dataloader å·²ç»åœ¨ä¸Šé¢å®šä¹‰å¥½äº†
# model = model.to(device) 

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================================
# 3. å¼€å§‹è®­ç»ƒå¾ªç¯ (å¸¦è¿›åº¦æ¡ç‰ˆ)
# ==========================================
num_epochs = 10 

for epoch in range(num_epochs):
    
    # --- é˜¶æ®µ A: è®­ç»ƒ (Training) ---
    model.train()
    running_loss = 0.0
    
    # <--- 2. è¿™é‡Œç”¨ tqdm åŒ…è£¹ train_dataloder
    # desc æ˜¯è¿›åº¦æ¡å·¦è¾¹çš„æ–‡å­—æè¿°
    # loop æ˜¯ä¸€ä¸ªæ–°çš„è¿­ä»£å™¨ï¼Œä»£æ›¿åŸæ¥çš„ train_dataloder
    loop = tqdm(train_dataloder, desc=f'Epoch [{epoch+1}/{num_epochs}]')
    
    for images, labels in loop:
        images = images.to(device)
        labels = labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        
        # <--- 3. å…³é”®ç‚¹ï¼šå®æ—¶æ›´æ–°è¿›åº¦æ¡å³ä¾§çš„å°å­— (æ˜¾ç¤ºå½“å‰ Batch çš„ Loss)
        loop.set_postfix(loss=loss.item())
    
    # è®¡ç®—æœ¬è½®å¹³å‡è®­ç»ƒæŸå¤±
    avg_train_loss = running_loss / len(train_dataloder)

    # --- é˜¶æ®µ B: æµ‹è¯•/éªŒè¯ (Validation) ---
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_dataloader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
    accuracy = 100 * correct / total
    
    # <--- 4. ä¸€è½®ç»“æŸåï¼Œç”¨ print æ‰“å°æœ€ç»ˆçš„å¹³å‡ç»“æœ
    # tqdm ä¼šè‡ªåŠ¨å¤„ç†æ¢è¡Œï¼Œä¸ç”¨æ‹…å¿ƒæ ¼å¼ä¹±æ‰
    print(f"Epoch {epoch+1} ç»“æŸ | å¹³å‡ Loss: {avg_train_loss:.4f} | éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

print("è®­ç»ƒå®Œæˆï¼")
```



## æäº¤

```python
# ä¼ªä»£ç é€»è¾‘
model.eval()
predictions = []

for images in test_loader:
    images = images.to(device)
    outputs = model(images)
    # å–æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªç´¢å¼•
    _, preds = torch.max(outputs, 1)
    predictions.extend(preds.tolist())

# æŠŠæ•°å­—è½¬å›å­—ç¬¦ä¸²
final_labels = [dataset.num_to_class[p] for p in predictions]

# ä¿å­˜åˆ° csv
submission_df = pd.read_csv("test.csv")
submission_df['label'] = final_labels
submission_df.to_csv("submission.csv", index=False)
```

## è·å¾—é«˜åˆ†

### ç¬¬å››é˜¶æ®µï¼šå¦‚ä½•è·å¾—é«˜åˆ† (Pro Tips)

å¦‚æœè·‘å®Œä¸Šé¢çš„æµç¨‹ï¼Œä½ å¤§æ¦‚èƒ½è¾¾åˆ° 80%-90% çš„å‡†ç¡®ç‡ã€‚æƒ³è¦å†²å‡» Leaderboard å‰æ’ï¼ˆ97%+ï¼‰ï¼Œä½ éœ€è¦ç”¨åˆ°ä»¥ä¸‹æŠ€å·§ï¼š

1. **TTA (Test Time Augmentation)**: åœ¨é¢„æµ‹æµ‹è¯•é›†æ—¶ï¼Œä¸è¦åªé¢„æµ‹ä¸€æ¬¡ã€‚æŠŠæµ‹è¯•å›¾ç‰‡ç¿»è½¬ä¸€ä¸‹é¢„æµ‹ä¸€æ¬¡ã€è°ƒäº®ä¸€ç‚¹é¢„æµ‹ä¸€æ¬¡ï¼Œæœ€åæŠŠè¿™å‡ æ¬¡é¢„æµ‹çš„ç»“æœ**å–å¹³å‡**ã€‚è¿™é€šå¸¸èƒ½æåˆ† 1%~2%ã€‚
2. **K-Fold Cross Validation (KæŠ˜äº¤å‰éªŒè¯)**: ä¸è¦åªæŠŠæ•°æ®åˆ‡æˆå›ºå®šçš„ 8:2ã€‚æŠŠæ•°æ®åˆ†æˆ 5 ä»½ï¼Œè®­ç»ƒ 5 ä¸ªæ¨¡å‹ï¼ˆæ¯ä¸ªæ¨¡å‹ç”¨ä¸åŒçš„ä»½åšéªŒè¯é›†ï¼‰ï¼Œæœ€åè®©è¿™ 5 ä¸ªæ¨¡å‹ä¸€èµ·æŠ•ç¥¨ã€‚
3. **å­¦ä¹ ç‡è°ƒæ•´ (Scheduler)**: ä½¿ç”¨ `CosineAnnealingLR`ã€‚åˆšå¼€å§‹å­¦ä¹ ç‡å¤§ä¸€ç‚¹ï¼Œåé¢è¶Šæ¥è¶Šå°ï¼Œè¿™æ ·æ¨¡å‹èƒ½æ”¶æ•›å¾—æ›´ç»†è‡´ã€‚
