# Module-0

## ğŸ“š è¯¾ç¨‹æ¦‚è¿°

æ¬¢è¿æ¥åˆ° LangChain Academyï¼

### èƒŒæ™¯ä»‹ç»

LangChain çš„ç›®æ ‡æ˜¯è®©æ„å»º LLM åº”ç”¨å˜å¾—ç®€å•ã€‚Agentï¼ˆä»£ç†ï¼‰æ˜¯ä¸€ç§å¯ä»¥æ„å»ºçš„ LLM åº”ç”¨ã€‚Agent ä¹‹æ‰€ä»¥å¤‡å—å…³æ³¨ï¼Œæ˜¯å› ä¸ºå®ƒä»¬å¯ä»¥è‡ªåŠ¨åŒ–å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚

ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œæ„å»ºèƒ½å¤Ÿå¯é æ‰§è¡Œè¿™äº›ä»»åŠ¡çš„ç³»ç»Ÿéå¸¸å›°éš¾ã€‚é€šè¿‡ä¸ç”¨æˆ·åˆä½œå°† Agent æŠ•å…¥ç”Ÿäº§ï¼Œæˆ‘ä»¬å­¦åˆ°äº†æ›´å¤šçš„æ§åˆ¶å’Œå¯è§‚æµ‹æ€§å¯¹äºæ„å»ºå¯é çš„ç³»ç»Ÿè‡³å…³é‡è¦ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº† [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) â€” ä¸€ä¸ªç”¨äºæ„å»ºå• Agent å’Œå¤š Agent åº”ç”¨çš„æ¡†æ¶ã€‚

### è¯¾ç¨‹ç»“æ„

è¯¾ç¨‹ç”±å¤šä¸ªæ¨¡å—ç»„æˆï¼Œæ¯ä¸ªæ¨¡å—ä¸“æ³¨äº LangGraph ç›¸å…³çš„ç‰¹å®šä¸»é¢˜ã€‚æ¯ä¸ªæ¨¡å—æ–‡ä»¶å¤¹åŒ…å«ä¸€ç³»åˆ—ç¬”è®°æœ¬ã€‚

### å‡†å¤‡å·¥ä½œ

å¼€å§‹ä¹‹å‰ï¼Œè¯·æŒ‰ç…§ `README` ä¸­çš„è¯´æ˜åˆ›å»ºç¯å¢ƒå¹¶å®‰è£…ä¾èµ–ã€‚

---

## ğŸ’¬ èŠå¤©æ¨¡å‹

åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨èŠå¤©æ¨¡å‹ï¼Œå®ƒæ¥æ”¶æ¶ˆæ¯åºåˆ—ä½œä¸ºè¾“å…¥å¹¶è¿”å›æ¶ˆæ¯ä½œä¸ºè¾“å‡ºã€‚LangChain é€šè¿‡[ç¬¬ä¸‰æ–¹é›†æˆ](https://docs.langchain.com/)æ”¯æŒè®¸å¤šæ¨¡å‹ã€‚

### è®¾ç½® API å¯†é’¥

```python
import os, getpass

def _set_env(var:  str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}:  ")

# _set_env("OPENAI_API_KEY")
```

### å¸¸è§å‚æ•°

èŠå¤©æ¨¡å‹æœ‰ä¸¤ä¸ªæœ€å¸¸è§çš„å‚æ•°ï¼š

- **`model`**: æ¨¡å‹åç§°
- **`temperature`**: é‡‡æ ·æ¸©åº¦ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
  - ä½æ¸©åº¦ï¼ˆæ¥è¿‘ 0ï¼‰ï¼šè¾“å‡ºæ›´ç¡®å®šå’Œæœ‰é’ˆå¯¹æ€§ï¼Œé€‚åˆéœ€è¦å‡†ç¡®æ€§çš„ä»»åŠ¡
  - é«˜æ¸©åº¦ï¼šè¾“å‡ºæ›´å…·åˆ›æ„å’Œå¤šæ ·æ€§ï¼Œé€‚åˆåˆ›æ„ä»»åŠ¡

### åˆå§‹åŒ–æ¨¡å‹

```python
from langchain.chat_models import init_chat_model

qwen = init_chat_model(
    model_provider="ollama",
    model="qwen3:8b",
    base_url="http://localhost:11434",
    temperature=0.7,
)
```

```python
# è¿™ä¸€å¥—å…¼å®¹æ€§æ›´å¼º
from langchain_ollama import ChatOllama
qwen = ChatOllama(
    model="qwen3:8b",
    temperature=0,
)
```



### ä¸»è¦æ–¹æ³•

- **`stream`**: æµå¼è¿”å›å“åº”çš„å—
- **`invoke`**: è°ƒç”¨é“¾æ‰§è¡Œè¾“å…¥

### ä½¿ç”¨æ¶ˆæ¯

èŠå¤©æ¨¡å‹æ¥æ”¶æ¶ˆæ¯ä½œä¸ºè¾“å…¥ã€‚æ¶ˆæ¯åŒ…å«ï¼š
- **role**: æè¿°è¯´è¯è€…çš„è§’è‰²
- **content**: æ¶ˆæ¯å†…å®¹

```python
from langchain_core.messages import HumanMessage

# åˆ›å»ºæ¶ˆæ¯
msg = HumanMessage(content="Hello world", name="Lance")

# æ¶ˆæ¯åˆ—è¡¨
messages = [msg]

# è°ƒç”¨æ¨¡å‹
qwen.invoke(messages)
```

### ç®€åŒ–ç”¨æ³•

å¯ä»¥ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²ï¼Œå®ƒä¼šè‡ªåŠ¨è½¬æ¢ä¸º `HumanMessage`ï¼š

```python
qwen.invoke("hello world")

```

### æ¨¡å‹ä¸€è‡´æ€§

æ‰€æœ‰èŠå¤©æ¨¡å‹çš„æ¥å£ä¸€è‡´ï¼Œé€šå¸¸åœ¨æ¯ä¸ªç¬”è®°æœ¬å¼€å§‹æ—¶åˆå§‹åŒ–ä¸€æ¬¡ã€‚è¿™æ ·å¯ä»¥è½»æ¾åœ°åœ¨ä¸åŒæä¾›å•†ä¹‹é—´åˆ‡æ¢ã€‚

---

## ğŸ” æœç´¢å·¥å…·

æœ¬è¯¾ç¨‹ä½¿ç”¨ [Tavily](https://tavily.com/)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸º LLM å’Œ RAG ä¼˜åŒ–çš„æœç´¢å¼•æ“ï¼Œè‡´åŠ›äºæä¾›é«˜æ•ˆã€å¿«é€Ÿå’ŒæŒä¹…çš„æœç´¢ç»“æœã€‚

### è®¾ç½® Tavily

```python
_set_env("TAVILY_API_KEY")

from langchain_tavily import TavilySearch

tavily_search = TavilySearch(max_results=3)

# æ‰§è¡Œæœç´¢
data = tavily_search.invoke({"query": "What is LangGraph?"})
search_docs = data.get("results", data)

# æŸ¥çœ‹æœç´¢ç»“æœ
search_docs
```

### æœç´¢ç»“æœç¤ºä¾‹

è¿”å›çš„æœç´¢ç»“æœåŒ…å«ï¼š
- **url**: æ¥æºç½‘å€
- **title**: æ ‡é¢˜
- **content**: å†…å®¹æ‘˜è¦
- **score**: ç›¸å…³æ€§å¾—åˆ†

---



# Module-1

## å¯¼åŒ…

```python
import os, getpass # å’Œé…ç½® key ç›¸å…³
from langchain_ollama import ChatOllama # æ¨¡å‹ç›¸å…³
from langsmith import traceable # LangSmith ç›¸å…³
from langgraph.graph import MessagesState # è¿½åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ ä»¥ MessagesState ä½œä¸ºçŠ¶æ€æ¥ä¼ é€’ å¯ä»¥ç‚¹è¿›å»çœ‹çœ‹æ ·å­
from langchain_core.messages import HumanMessage, SystemMessage # ç”¨æˆ·ä¿¡æ¯å’Œç³»ç»Ÿä¿¡æ¯
from langgraph.graph import START, END, StateGraph # åˆ›å»º å›¾ ç›¸å…³
from langgraph.prebuilt import tools_condition # å¦‚æœ LLM å†³å®šè°ƒç”¨å·¥å…·ï¼Œé€šå‘å« "tools" çš„èŠ‚ç‚¹ï¼Œå¦åˆ™å» END
from langgraph.prebuilt import ToolNode # å†…ç½®çš„ ToolNode ç»„ä»¶ï¼Œåªéœ€ä¼ å…¥å·¥å…·åˆ—è¡¨å³å¯åˆå§‹åŒ–å®ƒï¼Œç›¸å½“äºä¸€ä¸ªèŠ‚ç‚¹
from IPython.display import Image, display # å±•ç¤ºç›¸å…³
from langgraph.checkpoint.memory import MemorySaver # Agent memory ç›¸å…³
```

## LangSmith

è¾“å…¥API å†ç»™èŠ‚ç‚¹å’Œå·¥å…·åŠ ä¸Š @traceable å³å¯è¿½è¸ª

```python
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")
_set_env("LANGSMITH_API_KEY")
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "langchain-academy"
```



## æ¨¡å‹

```python
qwen = ChatOllama(
    model="qwen3:8b",
    temperature=0,
)
```



## å·¥å…·

```python
@traceable # LangSmith è¿½è¸ª
def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

# This will be a tool
@traceable
def add(a: int, b: int) -> int:
    """Adds a and b.

    Args:
        a: first int
        b: second int
    """
    return a + b

@traceable
def divide(a: int, b: int) -> float:
    """Divide a and b.

    Args:
        a: first int
        b: second int
    """
    return a / b

tools =  [add, multiply, divide]
```

## æ¨¡å‹ç»‘å®šå·¥å…·

æŠŠ Python å‡½æ•°é‡Œçš„ __doc__ å˜æˆ JSON è¯´æ˜ä¹¦ç»™ LLMï¼ŒLLM é€šè¿‡è¿™ä¸ª JSON æ¥è°ƒç”¨å·¥å…·

```python
llm_with_tools = qwen.bind_tools(tools) # parallel_tool_calls=False å…³é—­å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼ˆOllama æ²¡æœ‰è¿™ä¸ªå‚æ•°ï¼‰
```

**å±•ç¤ºï¼š**

```python
from langchain_core.utils.function_calling import convert_to_openai_tool
# é­”æ³•å°±åœ¨è¿™é‡Œï¼šå°†å‡½æ•°è½¬æ¢ä¸ºå·¥å…·æ ¼å¼
tool_json = convert_to_openai_tool(multiply)

import json
print(json.dumps(tool_json, indent=2, ensure_ascii=False))
```

```python
{
  "type": "function",
  "function": {
    "name": "multiply",
    "description": "è®¡ç®— a ä¹˜ b å°±ç”¨è¿™ä¸ªæ–¹æ³•",
    "parameters": {
      "properties": {
        "a": {
          "description": "first int",
          "type": "integer"
        },
        "b": {
          "description": "second int",
          "type": "integer"
        }
      },
      "required": [
        "a",
        "b"
      ],
      "type": "object"
    }
  }
}
```

## æ„å»ºå›¾



```python
# node
def tool_calling_llm(state: MessagesState):
    return {"messages": [llm_with_tools.invoke(state["messages"])]} # è°ƒç”¨æ¨¡å‹æ—¶ï¼Œä¼šè‡ªåŠ¨è¿”å›æ•°æ®ç±»å‹(AIMessage)

# Build graph
builder = StateGraph(MessagesState)
builder.add_node("tool_calling_llm", tool_calling_llm) # llm èŠ‚ç‚¹
builder.add_node("tools", ToolNode([multiply])) # å¢åŠ ä¸€ä¸ªå·¥å…·èŠ‚ç‚¹

builder.add_edge(START, "tool_calling_llm")
builder.add_conditional_edges(
    "tool_calling_llm",
    tools_condition, # è¦ä¹ˆé€šå‘ä¸€ä¸ª "tools" èŠ‚ç‚¹ï¼Œè¦ä¹ˆ é€šå‘ END  
)
builder.add_edge("tools", "tool_calling_llm")
graph = builder.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))
```

![å±å¹•æˆªå›¾ 2026-01-05 214248](C:\Users\34356\Desktop\md\resources\å±å¹•æˆªå›¾ 2026-01-05 214248.png)



## Agent memory

LangGraph å¯ä»¥ä½¿ç”¨æ£€æŸ¥ç‚¹æ¥è‡ªåŠ¨ä¿å­˜æ¯ä¸€æ­¥ä¹‹åçš„å›¾çŠ¶æ€ã€‚

è¿™ä¸ªå†…ç½®çš„æŒä¹…åŒ–å±‚ä¸ºæˆ‘ä»¬æä¾›äº†å†…å­˜ï¼Œä½¿ LangGraph èƒ½å¤Ÿä»ä¸Šæ¬¡çŠ¶æ€æ›´æ–°çš„ä½ç½®ç»§ç»­æ‰§è¡Œã€‚

æœ€å®¹æ˜“ä½¿ç”¨çš„æ£€æŸ¥ç‚¹ä¹‹ä¸€æ˜¯ `MemorySaver`ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºå­˜å‚¨å›¾çŠ¶æ€çš„å†…å­˜é”®å€¼å­˜å‚¨ã€‚

æˆ‘ä»¬åªéœ€è¦ä½¿ç”¨æ£€æŸ¥ç‚¹ç¼–è¯‘å›¾ï¼Œæˆ‘ä»¬çš„å›¾å°±æœ‰äº†å†…å­˜ï¼

```python
memory = MemorySaver()
react_graph_memory = builder.compile(checkpointer=memory)
```

å½“æˆ‘ä»¬ä½¿ç”¨å†…å­˜æ—¶ï¼Œéœ€è¦æŒ‡å®šä¸€ä¸ª `thread_id`ã€‚

è¿™ä¸ª `thread_id` å°†å­˜å‚¨æˆ‘ä»¬å›¾çš„çŠ¶æ€é›†åˆã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºæ„å›¾ï¼š

* æ£€æŸ¥ç‚¹åœ¨å›¾çš„æ¯ä¸€æ­¥å†™å…¥çŠ¶æ€
* è¿™äº›æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ä¸€ä¸ªçº¿ç¨‹ä¸­
* æˆ‘ä»¬ä»¥åå¯ä»¥ä½¿ç”¨ `thread_id` è®¿é—®è¯¥çº¿ç¨‹
* ![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)

**å»ºç«‹ä¸€ä¸ª checkpoint**

```python
# Specify a thread
config = {"configurable": {"thread_id": "1"}}

# Specify an input
messages = [HumanMessage(content="Add 3 and 4.")]

# Run
messages = react_graph_memory.invoke({"messages": messages},config)
for m in messages['messages']:
    m.pretty_print()
```

**è¿è¡Œæ—¶ï¼ŒæŠŠ checkpoint ä¹ŸåŠ å…¥åˆ° messages**

```python
messages = [HumanMessage(content="Multiply that by 2.")]
messages = react_graph_memory.invoke({"messages": messages}, config)
for m in messages['messages']:
    m.pretty_print()
```



# Module-2

## state-reducers

è‡ªå®šä¹‰ reducer

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰è‡ªå®šä¹‰ reducer é€»è¾‘æ¥åˆå¹¶åˆ—è¡¨ï¼Œå¹¶å¤„ç†è¾“å…¥ä¸­ä¸€ä¸ªæˆ–ä¸¤ä¸ªéƒ½ä¸º `None` çš„æƒ…å†µ

```python
def reduce_list(left: list | None, right: list | None) -> list:
    """Safely combine two lists, handling cases where either or both inputs might be None.

    Args:
        left (list | None): The first list to combine, or None.
        right (list | None): The second list to combine, or None.

    Returns:
        list: A new list containing all elements from both input lists.
               If an input is None, it's treated as an empty list.
    """
    if not left:
        left = []
    if not right:
        right = []
    return left + right

class DefaultState(TypedDict):
    foo: Annotated[list[int], add]

class CustomReducerState(TypedDict):
    foo: Annotated[list[int], reduce_list] 
```

 `MessagesState` æœ‰ä¸€ä¸ªå†…ç½®çš„ `messages` é”®

 å®ƒè¿˜æœ‰ä¸€ä¸ªå†…ç½®çš„ `add_messages` reducer æ¥å¤„ç†è¯¥é”®

è¿™ä¸¤è€…æ˜¯ç­‰æ•ˆçš„ã€‚

ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬å°†é€šè¿‡ `from langgraph.graph import MessagesState` æ¥ä½¿ç”¨ `MessagesState` ç±»ã€‚

```python
# æ–°å¢ä¿¡æ¯
from langgraph.graph.message import add_messages
from langchain_core.messages import AIMessage, HumanMessage

# Initial state
initial_messages = [AIMessage(content="Hello! How can I assist you?", name="Model"),
                    HumanMessage(content="I'm looking for information on marine biology.", name="Lance")
                   ]

# New message to add
new_message = AIMessage(content="Sure, I can help with that. What specifically are you interested in?", name="Model")

# Test
add_messages(initial_messages , new_message)
```

```python
# æ ¹æ® id é‡å†™ä¿¡æ¯
# Initial state
initial_messages = [AIMessage(content="Hello! How can I assist you?", name="Model", id="1"),
                    HumanMessage(content="I'm looking for information on marine biology.", name="Lance", id="2")
                   ]

# New message to add
new_message = HumanMessage(content="I'm looking for information on whales, specifically", name="Lance", id="2")

# Test
add_messages(initial_messages , new_message)
```

```python
# ç§»é™¤ä¿¡æ¯
from langchain_core.messages import RemoveMessage

# Message list
messages = [AIMessage("Hi.", name="Bot", id="1")]
messages.append(HumanMessage("Hi.", name="Lance", id="2"))
messages.append(AIMessage("So you said you were researching ocean mammals?", name="Bot", id="3"))
messages.append(HumanMessage("Yes, I know about whales. But what others should I learn about?", name="Lance", id="4"))

# Isolate messages to delete
delete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]]
print(delete_messages)
add_messages(messages , delete_messages)
```



## å¤šæ¨¡å¼

  ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨å›¾ä¸­ä½¿ç”¨ç‰¹å®šçš„ `input` å’Œ `output` æ¨¡å¼ã€‚

è¿™é‡Œï¼Œ`input` / `output` æ¨¡å¼å¯¹å›¾çš„è¾“å…¥å’Œè¾“å‡ºä¸­å…è®¸çš„é”®è¿›è¡Œ**è¿‡æ»¤**ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç±»å‹æç¤º `state: InputState` æ¥æŒ‡å®šæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥æ¨¡å¼ã€‚

å½“å›¾ä½¿ç”¨å¤šä¸ªæ¨¡å¼æ—¶ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ç±»å‹æç¤ºæ¥è¡¨æ˜ `answer_node` çš„è¾“å‡ºå°†è¢«è¿‡æ»¤ä¸º `OutputState`ã€‚

```python
class InputState(TypedDict):
    question: str

class OutputState(TypedDict):
    answer: str

class OverallState(TypedDict):
    question: str
    answer: str
    notes: str

def thinking_node(state: InputState):
    return {"answer": "bye", "notes": "... his is name is Lance"}

def answer_node(state: OverallState) -> OutputState:
    return {"answer": "bye Lance"}

graph = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
graph.add_node("answer_node", answer_node)
graph.add_node("thinking_node", thinking_node)
graph.add_edge(START, "thinking_node")
graph.add_edge("thinking_node", "answer_node")
graph.add_edge("answer_node", END)

graph = graph.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))

graph.invoke({"question":"hi"})
```

## è¿‡æ»¤å’Œä¿®å‰ªæ¶ˆæ¯

###  æ¶ˆæ¯è¿‡æ»¤

é€šè¿‡ `RemoveMessage` åˆ é™¤æ¶ˆæ¯

```python
from langchain_core.messages import RemoveMessage

# Nodes
def filter_messages(state: MessagesState):
    # Delete all but the 2 most recent messages
    # åˆ é™¤é™¤æœ€è¿‘ä¸¤æ¡æ¶ˆæ¯ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯ã€‚
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"messages": delete_messages}

def chat_model_node(state: MessagesState):    
    return {"messages": [llm.invoke(state["messages"])]}

# Build graph
builder = StateGraph(MessagesState)
builder.add_node("filter", filter_messages)
builder.add_node("chat_model", chat_model_node)
builder.add_edge(START, "filter")
builder.add_edge("filter", "chat_model")
builder.add_edge("chat_model", END)
graph = builder.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))
```

æŸ¥çœ‹æ•ˆæœï¼š

```python
# Message list with a preamble
messages = [AIMessage("Hi.", name="Bot", id="1")]
messages.append(HumanMessage("Hi.", name="Lance", id="2"))
messages.append(AIMessage("So you said you were researching ocean mammals?", name="Bot", id="3"))
messages.append(HumanMessage("Yes, I know about whales. But what others should I learn about?", name="Lance", id="4"))

# Invoke
output = graph.invoke({'messages': messages})
for m in output['messages']:
    m.pretty_print()
```



å¦‚æœä¸éœ€è¦æˆ–ä¸æƒ³ä¿®æ”¹å›¾çŠ¶æ€ï¼Œæ‚¨å¯ä»¥ç›´æ¥è¿‡æ»¤ä¼ é€’ç»™èŠå¤©æ¨¡å‹çš„æ¶ˆæ¯ã€‚

ä¾‹å¦‚ï¼Œåªéœ€å°†è¿‡æ»¤åçš„åˆ—è¡¨ï¼š`llm.invoke(messages[-1:])` ä¼ é€’ç»™æ¨¡å‹å³å¯ã€‚

```python
# Node
def chat_model_node(state: MessagesState):
    return {"messages": [llm.invoke(state["messages"][-1:])]}

# Build graph
builder = StateGraph(MessagesState)
builder.add_node("chat_model", chat_model_node)
builder.add_edge(START, "chat_model")
builder.add_edge("chat_model", END)
graph = builder.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))
```

### æ¶ˆæ¯ä¿®å‰ª

å¦ä¸€ç§æ–¹æ³•æ˜¯æ ¹æ®é¢„è®¾çš„è¯å…ƒæ•°é‡[ä¿®å‰ªæ¶ˆæ¯](https://docs.langchain.com/oss/python/langgraph/add-memory#trim-messages)ã€‚

è¿™ä¼šå°†æ¶ˆæ¯å†å²è®°å½•é™åˆ¶åœ¨æŒ‡å®šæ•°é‡çš„è¯å…ƒå†…ã€‚

è¿‡æ»¤ä»…è¿”å›ä»£ç†ä¹‹é—´æ¶ˆæ¯çš„åéªŒå­é›†ï¼Œè€Œä¿®å‰ªåˆ™é™åˆ¶äº†èŠå¤©æ¨¡å‹å¯ç”¨äºå“åº”çš„è¯å…ƒæ•°é‡ã€‚

è¯·å‚é˜…ä¸‹é¢çš„ `trim_messages`ã€‚

```python
from langchain_core.messages import trim_messages

# Node
def chat_model_node(state: MessagesState):
    messages = trim_messages(
            state["messages"],
            max_tokens=100,
            strategy="last",
            token_counter=ChatOpenAI(model="gpt-4o"),
            allow_partial=False,
        )
    return {"messages": [llm.invoke(messages)]}

# Build graph
builder = StateGraph(MessagesState)
builder.add_node("chat_model", chat_model_node)
builder.add_edge(START, "chat_model")
builder.add_edge("chat_model", END)
graph = builder.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))
```



## å¸¦æœ‰æ¶ˆæ¯æ‘˜è¦å’Œå¤–éƒ¨æ•°æ®åº“å†…å­˜çš„èŠå¤©æœºå™¨äºº

### å¯¼åŒ…

```python
import os, getpass # å’Œé…ç½® key ç›¸å…³
from langchain_ollama import ChatOllama # æ¨¡å‹ç›¸å…³
from langsmith import traceable # LangSmith ç›¸å…³
from langgraph.graph import MessagesState # è¿½åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ ä»¥ MessagesState ä½œä¸ºçŠ¶æ€æ¥ä¼ é€’ å¯ä»¥ç‚¹è¿›å»çœ‹çœ‹æ ·å­
from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage # ç”¨æˆ·ä¿¡æ¯å’Œç³»ç»Ÿä¿¡æ¯å’Œåˆ é™¤ä¿¡æ¯
from langgraph.graph import START, END, StateGraph # åˆ›å»º å›¾ ç›¸å…³
from langgraph.prebuilt import tools_condition # å¦‚æœ LLM å†³å®šè°ƒç”¨å·¥å…·ï¼Œé€šå‘å« "tools" çš„èŠ‚ç‚¹ï¼Œå¦åˆ™å» END è¿™éƒ¨åˆ†æ²¡ç”¨ä¸Š
from langgraph.prebuilt import ToolNode # å†…ç½®çš„ ToolNode ç»„ä»¶ï¼Œåªéœ€ä¼ å…¥å·¥å…·åˆ—è¡¨å³å¯åˆå§‹åŒ–å®ƒï¼Œç›¸å½“äºä¸€ä¸ªèŠ‚ç‚¹ è¿™éƒ¨åˆ†æ²¡ç”¨ä¸Š
from IPython.display import Image, display # å±•ç¤ºç›¸å…³
from langgraph.checkpoint.memory import MemorySaver # Agent memory ç›¸å…³ è¿™éƒ¨åˆ†å°±æ˜¯ç”¨çš„ SqliteSaver æ¥ä»£æ›¿çš„
import sqlite3 # ä½¿ç”¨å°å·§ã€å¿«é€Ÿã€æµè¡Œçš„æ•°æ®åº“ SQLite
from langgraph.checkpoint.sqlite import SqliteSaver # å’Œ memory ç›¸å…³
```

### å¯¼å…¥ç¯å¢ƒå˜é‡

```python
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# LangSmith ç›¸å…³
_set_env("LANGSMITH_API_KEY")
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "langchain-academy"
```

### ä½¿ç”¨ SQLite æ¥æŒä¹…åŒ– æ•°æ®

```python
# In memory
# åœ¨å†…å­˜ä¸­åˆ›å»ºä¸€ä¸ªæ•°æ®åº“ï¼Œå…³é—­çº¿ç¨‹æ£€æµ‹ï¼Œå…±äº«è¿™ä¸ªæ•°æ®åº“
# conn = sqlite3.connect(":memory:", check_same_thread = False)

# å¦‚æœæä¾›è·¯å¾„ï¼Œå°±ä¼šåˆ›å»ºä¸€ä¸ªæ•°æ®åº“
db_path = r"D:\code\langchain\langchain-academy\module-2\state_db\test.db"
conn = sqlite3.connect(db_path, check_same_thread=False)
```

å®šä¹‰ memory å’Œæ•°æ®åº“ç»‘å®š

```python
memory = SqliteSaver(conn)
```

### å®šä¹‰èŠå¤©æœºå™¨äºº

```python
qwen = ChatOllama(
    model="qwen3:8b",
    temperature=0,
)

# è‡ªå®šä¹‰çŠ¶æ€ï¼ŒåŠ ä¸Šä¸ªæ€»ç»“å­—æ®µ
class State(MessagesState):
    summary: str

# å®šä¹‰è°ƒç”¨æ¨¡å‹çš„é€»è¾‘
def call_model(state: State):
    # å¦‚æœå­˜åœ¨æ‘˜è¦ï¼Œè·å¾—æ‘˜è¦
    summary = state.get("summary","")

    # å¦‚æœæœ‰æ‘˜è¦ï¼ŒåŠ å…¥è¿›å»
    if summary:

        # å°†æ‘˜è¦æ·»åŠ è¿›ç³»ç»Ÿæç¤ºä¿¡æ¯
        system_message = f"å…ˆå‰å¯¹è¯çš„æ€»ç»“ï¼š{summary}"

        # å°†æ‘˜è¦æ·»åŠ åˆ°ä»»ä½•è¾ƒæ–°çš„æ¶ˆæ¯ä¸­(æ”¾åœ¨æœ€å‰é¢ï¼Œæœ€å…ˆè¯»åˆ°ç³»ç»Ÿæç¤º)
        messages = [SystemMessage(content=system_message)] + state["messages"]

    else:
        messages = state["messages"]

    response = qwen.invoke(messages)
    return {"messages": response}

# æ€»ç»“å¯¹è¯å†…å®¹
def summarize_conversation(state: State):

    # é¦–å…ˆï¼Œè¦è·å¾—ä»»ä½•å­˜åœ¨çš„æ‘˜è¦
    summary = state.get("summary","")

    # åˆ›å»ºè‡ªå·±çš„æ‘˜è¦æ¨¡æ¿
    if summary:

        # å·²ç»å­˜åœ¨æ‘˜è¦
        summary_message = (
            f"è¿™æ˜¯è¿„ä»Šä¸ºæ­¢çš„å¯¹è¯æ‘˜è¦ï¼š{summary}\n\n"
            "è¯·æ ¹æ®ä»¥ä¸Šæ–°æ¶ˆæ¯è¡¥å……æ‘˜è¦ï¼š"
        )

    else:
        summary_message = "è¯·æ€»ç»“ä»¥ä¸Šå¯¹è¯å†…å®¹ï¼š"

    # åœ¨å†å²è®°å½•ä¸­æ·»åŠ æç¤º
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = qwen.invoke(messages)

    # åˆ é™¤é™¤æœ€è¿‘ä¸¤æ¡æ¶ˆæ¯ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    # æŠŠå‰é¢çš„æ€»ç»“äº†å°±ä¸éœ€è¦ç•™ä¸‹æ¥äº†
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    # æŠŠæ‘˜è¦å­˜èµ·æ¥ï¼Œç„¶ååˆ é™¤å¤šä½™çš„æ¶ˆæ¯
    return {"summary": response.content, "messages": delete_messages}

# å†³å®šæ˜¯ç»“æŸå¯¹è¯è¿˜æ˜¯æ€»ç»“å¯¹è¯
def should_continue(state: State) -> Literal ["summarize_conversation", END]:
    """
    è¿”å›è¦æ‰§è¡Œçš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
    """
    messages = state["messages"]

    # å¦‚æœè¶…è¿‡å…­æ¡ä¿¡æ¯ï¼Œå°±å¯¹å¯¹è¯è¿›è¡Œæ€»ç»“
    if len(messages) > 6:
        return "summarize_conversation"

    return END
```



### ä½¿ç”¨ SQLite Checkpointer æ¥æ„å»ºå›¾

```python
# å®šä¹‰ ä¸€ä¸ª æ–°çš„ å›¾
workflow = StateGraph(State)
workflow.add_node("conversation", call_model)
workflow.add_node(summarize_conversation)

# è®¾ç½®å…¥å£ç‚¹ä¸º conversation
workflow.add_edge(START, "conversation")
workflow.add_conditional_edges("conversation", should_continue)
workflow.add_edge("conversation", END)

# ç»„åˆ
graph = workflow.compile(checkpointer=memory)
display(Image(graph.get_graph().draw_mermaid_png()))
```



### æµ‹è¯•æ•ˆæœ

åˆ›å»ºä¸€ä¸ªçº¿ç¨‹å¹¶å¤šæ¬¡è°ƒç”¨

```python
# åˆ›å»ºä¸€ä¸ªçº¿ç¨‹
config = {"configurable": {"thread_id": "1"}}

# å¼€å§‹å¯¹è¯
input_message = HumanMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯æ–‡è½¶")
output = graph.invoke({"messages": [input_message]}, config) 
for m in output['messages'][-1:]:
    m.pretty_print()

input_message = HumanMessage(content="ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ")
output = graph.invoke({"messages": [input_message]}, config) 
for m in output['messages'][-1:]:
    m.pretty_print()

input_message = HumanMessage(content="æˆ‘å–œæ¬¢ç©åšå¾·ä¹‹é—¨3ï¼")
output = graph.invoke({"messages": [input_message]}, config) 
for m in output['messages'][-1:]:
    m.pretty_print()
```

æŸ¥çœ‹æ‰€æœ‰èŠå¤©è®°å½•

```python
for m in output['messages']:
    m.pretty_print()
```

ç¡®è®¤ä¸€ä¸‹çŠ¶æ€æ˜¯å¦å·²ç»åœ¨æœ¬åœ°ä¿å­˜

å¯ä»¥é‡å¯å†…æ ¸åå†æ¬¡è°ƒç”¨è¯•è¯•

```python
config = {"configurable": {"thread_id": "1"}}
graph_state = graph.get_state(config)
graph_state
```

é‡å¯åæ‰“å°å…¨éƒ¨å†å²è®°å½•

```python
# 1. è·å– snapshot å¯¹è±¡ (åŒ…è£¹)
graph_state = graph.get_state(config)

# 2. æ‰“å¼€åŒ…è£¹ï¼Œæ‹¿å‡º values å­—å…¸ (é‡Œé¢çš„ä¸œè¥¿)
all_values = graph_state.values 
# æ­¤æ—¶ all_values ç±»ä¼¼ï¼š {'messages': [HumanMessage(...), AIMessage(...)]}

# 3. ä»å­—å…¸é‡Œå–å‡º "messages" åˆ—è¡¨
chat_history = all_values["messages"]

# --- æ‰“å°å‡ºæ¥çœ‹çœ‹ ---
for msg in chat_history:
    msg.pretty_print() # LangChain è‡ªå¸¦çš„æ¼‚äº®æ‰“å°æ–¹æ³•
```

