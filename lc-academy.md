# Module-0

## ğŸ“š è¯¾ç¨‹æ¦‚è¿°

æ¬¢è¿æ¥åˆ° LangChain Academyï¼

### èƒŒæ™¯ä»‹ç»

LangChain çš„ç›®æ ‡æ˜¯è®©æ„å»º LLM åº”ç”¨å˜å¾—ç®€å•ã€‚Agentï¼ˆä»£ç†ï¼‰æ˜¯ä¸€ç§å¯ä»¥æ„å»ºçš„ LLM åº”ç”¨ã€‚Agent ä¹‹æ‰€ä»¥å¤‡å—å…³æ³¨ï¼Œæ˜¯å› ä¸ºå®ƒä»¬å¯ä»¥è‡ªåŠ¨åŒ–å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚

ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œæ„å»ºèƒ½å¤Ÿå¯é æ‰§è¡Œè¿™äº›ä»»åŠ¡çš„ç³»ç»Ÿéå¸¸å›°éš¾ã€‚é€šè¿‡ä¸ç”¨æˆ·åˆä½œå°† Agent æŠ•å…¥ç”Ÿäº§ï¼Œæˆ‘ä»¬å­¦åˆ°äº†æ›´å¤šçš„æ§åˆ¶å’Œå¯è§‚æµ‹æ€§å¯¹äºæ„å»ºå¯é çš„ç³»ç»Ÿè‡³å…³é‡è¦ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº† [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) â€” ä¸€ä¸ªç”¨äºæ„å»ºå• Agent å’Œå¤š Agent åº”ç”¨çš„æ¡†æ¶ã€‚

### è¯¾ç¨‹ç»“æ„

è¯¾ç¨‹ç”±å¤šä¸ªæ¨¡å—ç»„æˆï¼Œæ¯ä¸ªæ¨¡å—ä¸“æ³¨äº LangGraph ç›¸å…³çš„ç‰¹å®šä¸»é¢˜ã€‚æ¯ä¸ªæ¨¡å—æ–‡ä»¶å¤¹åŒ…å«ä¸€ç³»åˆ—ç¬”è®°æœ¬ã€‚

### å‡†å¤‡å·¥ä½œ

å¼€å§‹ä¹‹å‰ï¼Œè¯·æŒ‰ç…§ `README` ä¸­çš„è¯´æ˜åˆ›å»ºç¯å¢ƒå¹¶å®‰è£…ä¾èµ–ã€‚

---

## ğŸ’¬ èŠå¤©æ¨¡å‹

åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨èŠå¤©æ¨¡å‹ï¼Œå®ƒæ¥æ”¶æ¶ˆæ¯åºåˆ—ä½œä¸ºè¾“å…¥å¹¶è¿”å›æ¶ˆæ¯ä½œä¸ºè¾“å‡ºã€‚LangChain é€šè¿‡[ç¬¬ä¸‰æ–¹é›†æˆ](https://docs.langchain.com/)æ”¯æŒè®¸å¤šæ¨¡å‹ã€‚

### è®¾ç½® API å¯†é’¥

```python
import os, getpass

def _set_env(var:  str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}:  ")

# _set_env("OPENAI_API_KEY")
```

### å¸¸è§å‚æ•°

èŠå¤©æ¨¡å‹æœ‰ä¸¤ä¸ªæœ€å¸¸è§çš„å‚æ•°ï¼š

- **`model`**: æ¨¡å‹åç§°
- **`temperature`**: é‡‡æ ·æ¸©åº¦ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
  - ä½æ¸©åº¦ï¼ˆæ¥è¿‘ 0ï¼‰ï¼šè¾“å‡ºæ›´ç¡®å®šå’Œæœ‰é’ˆå¯¹æ€§ï¼Œé€‚åˆéœ€è¦å‡†ç¡®æ€§çš„ä»»åŠ¡
  - é«˜æ¸©åº¦ï¼šè¾“å‡ºæ›´å…·åˆ›æ„å’Œå¤šæ ·æ€§ï¼Œé€‚åˆåˆ›æ„ä»»åŠ¡

### åˆå§‹åŒ–æ¨¡å‹

```python
from langchain.chat_models import init_chat_model

qwen = init_chat_model(
    model_provider="ollama",
    model="qwen3:8b",
    base_url="http://localhost:11434",
    temperature=0.7,
)
```

```python
# è¿™ä¸€å¥—å…¼å®¹æ€§æ›´å¼º
from langchain_ollama import ChatOllama
qwen = ChatOllama(
    model="qwen3:8b",
    temperature=0,
)
```



### ä¸»è¦æ–¹æ³•

- **`stream`**: æµå¼è¿”å›å“åº”çš„å—
- **`invoke`**: è°ƒç”¨é“¾æ‰§è¡Œè¾“å…¥

### ä½¿ç”¨æ¶ˆæ¯

èŠå¤©æ¨¡å‹æ¥æ”¶æ¶ˆæ¯ä½œä¸ºè¾“å…¥ã€‚æ¶ˆæ¯åŒ…å«ï¼š
- **role**: æè¿°è¯´è¯è€…çš„è§’è‰²
- **content**: æ¶ˆæ¯å†…å®¹

```python
from langchain_core.messages import HumanMessage

# åˆ›å»ºæ¶ˆæ¯
msg = HumanMessage(content="Hello world", name="Lance")

# æ¶ˆæ¯åˆ—è¡¨
messages = [msg]

# è°ƒç”¨æ¨¡å‹
qwen.invoke(messages)
```

### ç®€åŒ–ç”¨æ³•

å¯ä»¥ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²ï¼Œå®ƒä¼šè‡ªåŠ¨è½¬æ¢ä¸º `HumanMessage`ï¼š

```python
qwen.invoke("hello world")

```

### æ¨¡å‹ä¸€è‡´æ€§

æ‰€æœ‰èŠå¤©æ¨¡å‹çš„æ¥å£ä¸€è‡´ï¼Œé€šå¸¸åœ¨æ¯ä¸ªç¬”è®°æœ¬å¼€å§‹æ—¶åˆå§‹åŒ–ä¸€æ¬¡ã€‚è¿™æ ·å¯ä»¥è½»æ¾åœ°åœ¨ä¸åŒæä¾›å•†ä¹‹é—´åˆ‡æ¢ã€‚

---

## ğŸ” æœç´¢å·¥å…·

æœ¬è¯¾ç¨‹ä½¿ç”¨ [Tavily](https://tavily.com/)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸º LLM å’Œ RAG ä¼˜åŒ–çš„æœç´¢å¼•æ“ï¼Œè‡´åŠ›äºæä¾›é«˜æ•ˆã€å¿«é€Ÿå’ŒæŒä¹…çš„æœç´¢ç»“æœã€‚

### è®¾ç½® Tavily

```python
_set_env("TAVILY_API_KEY")

from langchain_tavily import TavilySearch

tavily_search = TavilySearch(max_results=3)

# æ‰§è¡Œæœç´¢
data = tavily_search.invoke({"query": "What is LangGraph?"})
search_docs = data.get("results", data)

# æŸ¥çœ‹æœç´¢ç»“æœ
search_docs
```

### æœç´¢ç»“æœç¤ºä¾‹

è¿”å›çš„æœç´¢ç»“æœåŒ…å«ï¼š
- **url**: æ¥æºç½‘å€
- **title**: æ ‡é¢˜
- **content**: å†…å®¹æ‘˜è¦
- **score**: ç›¸å…³æ€§å¾—åˆ†

---



# Module-1

## å¯¼åŒ…

```python
import os, getpass # å’Œé…ç½® key ç›¸å…³
from langchain_ollama import ChatOllama # æ¨¡å‹ç›¸å…³
from langsmith import traceable # LangSmith ç›¸å…³
from langgraph.graph import MessagesState # è¿½åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ ä»¥ MessagesState ä½œä¸ºçŠ¶æ€æ¥ä¼ é€’
from langchain_core.messages import HumanMessage, SystemMessage # ç”¨æˆ·ä¿¡æ¯å’Œç³»ç»Ÿä¿¡æ¯
from langgraph.graph import START, END, StateGraph # åˆ›å»º å›¾ ç›¸å…³
from langgraph.prebuilt import tools_condition # å¦‚æœ LLM å†³å®šè°ƒç”¨å·¥å…·ï¼Œé€šå‘å« "tools" çš„èŠ‚ç‚¹ï¼Œå¦åˆ™å» END
from langgraph.prebuilt import ToolNode # å†…ç½®çš„ ToolNode ç»„ä»¶ï¼Œåªéœ€ä¼ å…¥å·¥å…·åˆ—è¡¨å³å¯åˆå§‹åŒ–å®ƒï¼Œç›¸å½“äºä¸€ä¸ªèŠ‚ç‚¹
from IPython.display import Image, display # å±•ç¤ºç›¸å…³
from langgraph.checkpoint.memory import MemorySaver # Agent memory ç›¸å…³
```

## LangSmith

è¾“å…¥API å†ç»™èŠ‚ç‚¹å’Œå·¥å…·åŠ ä¸Š @traceable å³å¯è¿½è¸ª

```python
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")
_set_env("LANGSMITH_API_KEY")
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "langchain-academy"
```



## æ¨¡å‹

```python
qwen = ChatOllama(
    model="qwen3:8b",
    temperature=0,
)
```



## å·¥å…·

```python
@traceable # LangSmith è¿½è¸ª
def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

# This will be a tool
@traceable
def add(a: int, b: int) -> int:
    """Adds a and b.

    Args:
        a: first int
        b: second int
    """
    return a + b

@traceable
def divide(a: int, b: int) -> float:
    """Divide a and b.

    Args:
        a: first int
        b: second int
    """
    return a / b

tools =  [add, multiply, divide]
```

## æ¨¡å‹ç»‘å®šå·¥å…·

æŠŠ Python å‡½æ•°é‡Œçš„ __doc__ å˜æˆ JSON è¯´æ˜ä¹¦ç»™ LLMï¼ŒLLM é€šè¿‡è¿™ä¸ª JSON æ¥è°ƒç”¨å·¥å…·

```python
llm_with_tools = qwen.bind_tools(tools) # parallel_tool_calls=False å…³é—­å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼ˆOllama æ²¡æœ‰è¿™ä¸ªå‚æ•°ï¼‰
```

**å±•ç¤ºï¼š**

```python
from langchain_core.utils.function_calling import convert_to_openai_tool
# é­”æ³•å°±åœ¨è¿™é‡Œï¼šå°†å‡½æ•°è½¬æ¢ä¸ºå·¥å…·æ ¼å¼
tool_json = convert_to_openai_tool(multiply)

import json
print(json.dumps(tool_json, indent=2, ensure_ascii=False))
```

```python
{
  "type": "function",
  "function": {
    "name": "multiply",
    "description": "è®¡ç®— a ä¹˜ b å°±ç”¨è¿™ä¸ªæ–¹æ³•",
    "parameters": {
      "properties": {
        "a": {
          "description": "first int",
          "type": "integer"
        },
        "b": {
          "description": "second int",
          "type": "integer"
        }
      },
      "required": [
        "a",
        "b"
      ],
      "type": "object"
    }
  }
}
```

## æ„å»ºå›¾



```python
# node
def tool_calling_llm(state: MessagesState):
    return {"messages": [llm_with_tools.invoke(state["messages"])]} # è°ƒç”¨æ¨¡å‹æ—¶ï¼Œä¼šè‡ªåŠ¨è¿”å›æ•°æ®ç±»å‹(AIMessage)

# Build graph
builder = StateGraph(MessagesState)
builder.add_node("tool_calling_llm", tool_calling_llm) # llm èŠ‚ç‚¹
builder.add_node("tools", ToolNode([multiply])) # å¢åŠ ä¸€ä¸ªå·¥å…·èŠ‚ç‚¹

builder.add_edge(START, "tool_calling_llm")
builder.add_conditional_edges(
    "tool_calling_llm",
    tools_condition, # è¦ä¹ˆé€šå‘ä¸€ä¸ª "tools" èŠ‚ç‚¹ï¼Œè¦ä¹ˆ é€šå‘ END  
)
builder.add_edge("tools", "tool_calling_llm")
graph = builder.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))
```

![å±å¹•æˆªå›¾ 2026-01-05 214248](C:\Users\34356\Desktop\md\resources\å±å¹•æˆªå›¾ 2026-01-05 214248.png)



## Agent memory

LangGraph å¯ä»¥ä½¿ç”¨æ£€æŸ¥ç‚¹æ¥è‡ªåŠ¨ä¿å­˜æ¯ä¸€æ­¥ä¹‹åçš„å›¾çŠ¶æ€ã€‚

è¿™ä¸ªå†…ç½®çš„æŒä¹…åŒ–å±‚ä¸ºæˆ‘ä»¬æä¾›äº†å†…å­˜ï¼Œä½¿ LangGraph èƒ½å¤Ÿä»ä¸Šæ¬¡çŠ¶æ€æ›´æ–°çš„ä½ç½®ç»§ç»­æ‰§è¡Œã€‚

æœ€å®¹æ˜“ä½¿ç”¨çš„æ£€æŸ¥ç‚¹ä¹‹ä¸€æ˜¯ `MemorySaver`ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºå­˜å‚¨å›¾çŠ¶æ€çš„å†…å­˜é”®å€¼å­˜å‚¨ã€‚

æˆ‘ä»¬åªéœ€è¦ä½¿ç”¨æ£€æŸ¥ç‚¹ç¼–è¯‘å›¾ï¼Œæˆ‘ä»¬çš„å›¾å°±æœ‰äº†å†…å­˜ï¼

```python
memory = MemorySaver()
react_graph_memory = builder.compile(checkpointer=memory)
```

å½“æˆ‘ä»¬ä½¿ç”¨å†…å­˜æ—¶ï¼Œéœ€è¦æŒ‡å®šä¸€ä¸ª `thread_id`ã€‚

è¿™ä¸ª `thread_id` å°†å­˜å‚¨æˆ‘ä»¬å›¾çš„çŠ¶æ€é›†åˆã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºæ„å›¾ï¼š

* æ£€æŸ¥ç‚¹åœ¨å›¾çš„æ¯ä¸€æ­¥å†™å…¥çŠ¶æ€
* è¿™äº›æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ä¸€ä¸ªçº¿ç¨‹ä¸­
* æˆ‘ä»¬ä»¥åå¯ä»¥ä½¿ç”¨ `thread_id` è®¿é—®è¯¥çº¿ç¨‹
* ![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)

**å»ºç«‹ä¸€ä¸ª checkpoint**

```python
# Specify a thread
config = {"configurable": {"thread_id": "1"}}

# Specify an input
messages = [HumanMessage(content="Add 3 and 4.")]

# Run
messages = react_graph_memory.invoke({"messages": messages},config)
for m in messages['messages']:
    m.pretty_print()
```

**è¿è¡Œæ—¶ï¼ŒæŠŠ checkpoint ä¹ŸåŠ å…¥åˆ° messages**

```python
messages = [HumanMessage(content="Multiply that by 2.")]
messages = react_graph_memory.invoke({"messages": messages}, config)
for m in messages['messages']:
    m.pretty_print()
```

